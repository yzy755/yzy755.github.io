<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>易泽源的博客</title>
  <subtitle>一天进步一点 程序员由入门到放弃</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yzy755.github.io/"/>
  <updated>2017-05-12T09:05:43.482Z</updated>
  <id>http://yzy755.github.io/</id>
  
  <author>
    <name>易泽源</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>基于python+flask+mysql的web开发初探</title>
    <link href="http://yzy755.github.io/2017/05/12/%E5%9F%BA%E4%BA%8Epython+flask+mysql%E7%9A%84web%E5%BC%80%E5%8F%91%E5%88%9D%E6%8E%A2/"/>
    <id>http://yzy755.github.io/2017/05/12/基于python+flask+mysql的web开发初探/</id>
    <published>2017-05-12T07:40:53.000Z</published>
    <updated>2017-05-12T09:05:43.482Z</updated>
    
    <content type="html"><![CDATA[<p>由于用Python开发一个Web框架十分容易，所以Python有上百个开源的Web框架，当下流行的是Flask,Django等<br>Flask是一个使用 Python 编写的轻量级 Web 应用框架。其 WSGI 工具箱采用 Werkzeug ，模板引擎则使用 Jinja2 。<br><a id="more"></a><br>搭建的大致步骤和常见的问题：</p>
<p>1.安装anaconda，anaconda里面集成了很多关于python科学计算的第三方库，主要是安装方便，而python是一个编译器，如果不使用anaconda，那么安装起来会比较痛苦，各个库之间的依赖性就很难连接的很好</p>
<p>2.安装mysql</p>
<p>3.安装mysql-python，命令:pip install MySql-python<br>    错误1：Microsoft Visual C++ 14.0 is required.<br>    升级.NET Framework 版本到4.5.1<br>    从错误提示的url下载，下载后安装visualcppbuildtools_full</p>
<p>3.安装mysql connector, 下载地址：<a href="https://dev.mysql.com/downloads/connector/python/" target="_blank" rel="external">https://dev.mysql.com/downloads/connector/python/</a><br>我的系统为64， 下载的是mysql-connector-python-2.1.5-py3.4-winx64，还是报错，后来注意到寻找的目录是Program Files (x86)，又下了个32位的安装。错误过去了。但是又来新的错误：<br>    LINK : error LNK2001: unresolved external symbol _DllMainCRTStartup<br>    build\lib.win-amd64-3.5_mysql.cp35-win_amd64.pyd : fatal error LNK1120: 56 unresolved externalserror: command ‘C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\x86_amd64\link.exe’ failed with exit status 1120<br>5.安装第三方包pymysql。下载地址：<a href="https://pypi.python.org/pypi/PyMySQL#downloads" target="_blank" rel="external">https://pypi.python.org/pypi/PyMySQL#downloads</a><br>下载PyMySQL-0.7.11-py2.py3-none-any.whl，使用pip install PyMySQL-0.7.11-py2.py3-none-any.whl 即可，再执行pip install MySql-python就成功了</p>
<p>python引入mysql方法：<br>import pymysql as MySQLdb</p>
<p>首次尝试python的web开发：<br>源码:</p>
<pre><code>db.py:

    host=&quot;127.0.0.1&quot;
    user=&quot;root&quot;
    password=&quot;123456&quot;
    charset=&quot;utf8&quot;
    database=&quot;test&quot;
    port=3306

service.py:

    import pymysql as MySQLdb
    import sys
    import db
    class AService(object):
        def getA(self,id):
            conn = MySQLdb.connect(host=db.host,user=db.user,passwd=db.password,port=db.port,db=db.database,charset=db.charset)
            result=[]
            try:
                cursor = conn.cursor()
                cursor.execute(&quot;select id,sname from student where id=&apos;%d&apos;&quot;%(id))
                result = cursor.fetchone()

            finally:
                cursor.close()
                conn.close()
            return result

hello.py:

    from flask import Flask, jsonify
    import service
    import sys

    app = Flask(__name__)

    @app.route(&apos;/service&apos;, methods=[&apos;GET&apos;])
    def getSerivice():
            mservice=service.AService()
            result = mservice.getA(1)
            json = &quot;&quot;
            json +=&quot;{&quot;
            json +=&quot;&apos;id&apos;:&quot;+str(result[0])+&quot;,&quot;
            json +=&quot;&apos;sname&apos;:&apos;&quot;+result[1]+&quot;&apos;&quot;
            json +=&quot;}&quot;
            return json;
    if __name__==&quot;__main__&quot;:
        app.run()
</code></pre><p>在cmd中执行 python hello.py<br>打开浏览器 <a href="http://localhost:5000" target="_blank" rel="external">http://localhost:5000</a><br>出现 {‘id’:1,’sname’:’yizeyuan’}<br>成功！</p>
<p>这个例子太简单，实际上可以像平时javaweb开发一样使用MVC模式，上面的hello.py是控制器，service.py是model,而view我们同样可以使用html，Flask使用jinja2渲染<br>使用方式：<br>    from flask import Flask, request, render_template #加上render_template<br>当请求处理完成返回一个模板以及变量<br>    return render_template(‘index.html’, message=’欢迎使用flask’, username=’yizeyuan’)<br>首页的html代码为：</p>
<pre><code>&lt;html&gt;
    &lt;head&gt;
      &lt;title&gt;index&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
      &lt;h1 style=&quot;font-style:italic&quot;&gt;Home&lt;/h1&gt;
      {{message}}
      欢迎你，{{username}}
    &lt;/body&gt;
&lt;/html&gt;
</code></pre><p>很像angularjs有木有！</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由于用Python开发一个Web框架十分容易，所以Python有上百个开源的Web框架，当下流行的是Flask,Django等&lt;br&gt;Flask是一个使用 Python 编写的轻量级 Web 应用框架。其 WSGI 工具箱采用 Werkzeug ，模板引擎则使用 Jinja2 。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yzy755.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="python" scheme="http://yzy755.github.io/tags/python/"/>
    
      <category term="web开发" scheme="http://yzy755.github.io/tags/web%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow搭建和TensorBoard可视化的使用</title>
    <link href="http://yzy755.github.io/2017/05/09/TensorFlow%E6%90%AD%E5%BB%BA/"/>
    <id>http://yzy755.github.io/2017/05/09/TensorFlow搭建/</id>
    <published>2017-05-09T10:17:53.000Z</published>
    <updated>2017-05-12T09:02:10.816Z</updated>
    
    <content type="html"><![CDATA[<p>TensorFlow™ 是一个采用数据流图（data flow graphs），用于数值计算的开源软件库。节点（Nodes）在图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor）。它灵活的架构让你可以在多种平台上展开计算，例如台式计算机中的一个或多个CPU（或GPU），服务器，移动设备等等。TensorFlow 最初由Google大脑小组（隶属于Google机器智能研究机构）的研究员和工程师们开发出来，用于机器学习和深度神经网络方面的研究，但这个系统的通用性使其也可广泛用于其他计算领域。<br><a id="more"></a><br>1.Centos6.x默认使用的Python版本为2.6.6，而TensorFlow需要的Python版本起步为2.7以上，首先升级Python版本，已2.7.3为例：</p>
<h1 id="wget-https-www-python-org-ftp-python-2-7-3-Python-2-7-3-tgz"><a href="#wget-https-www-python-org-ftp-python-2-7-3-Python-2-7-3-tgz" class="headerlink" title="wget https://www.python.org/ftp/python/2.7.3/Python-2.7.3.tgz"></a>wget <a href="https://www.python.org/ftp/python/2.7.3/Python-2.7.3.tgz" target="_blank" rel="external">https://www.python.org/ftp/python/2.7.3/Python-2.7.3.tgz</a></h1><h1 id="tar-xvjf-Python-2-7-3-tar-bz2"><a href="#tar-xvjf-Python-2-7-3-tar-bz2" class="headerlink" title="tar -xvjf Python-2.7.3.tar.bz2"></a>tar -xvjf Python-2.7.3.tar.bz2</h1><h1 id="mkdir-usr-local-python27"><a href="#mkdir-usr-local-python27" class="headerlink" title="mkdir /usr/local/python27"></a>mkdir /usr/local/python27</h1><h1 id="configure-–prefix-usr-local-python27"><a href="#configure-–prefix-usr-local-python27" class="headerlink" title="./configure –prefix=/usr/local/python27"></a>./configure –prefix=/usr/local/python27</h1><h1 id="Make"><a href="#Make" class="headerlink" title="Make"></a>Make</h1><h1 id="make-install"><a href="#make-install" class="headerlink" title="make install"></a>make install</h1><p>以上安装好了2.7.3，但此时没有覆盖老版本，再将原来/usr/bin/python链接改为别的名字：</p>
<h1 id="mv-usr-bin-python-usr-bin-python-old"><a href="#mv-usr-bin-python-usr-bin-python-old" class="headerlink" title="mv /usr/bin/python /usr/bin/python_old"></a>mv /usr/bin/python /usr/bin/python_old</h1><p>再建立新版本python的软链接：</p>
<h1 id="ln-s-usr-local-python27-bin-python2-7-usr-bin-python"><a href="#ln-s-usr-local-python27-bin-python2-7-usr-bin-python" class="headerlink" title="ln -s /usr/local/python27/bin/python2.7 /usr/bin/python"></a>ln -s /usr/local/python27/bin/python2.7 /usr/bin/python</h1><p>再输入 </p>
<h1 id="python-–version"><a href="#python-–version" class="headerlink" title="python –version"></a>python –version</h1><p>会出现2.7.3则说明成功</p>
<p>但是，基于Python编写的yum不兼容2.7以上版本，所以修改yum命令指定的Python版本:</p>
<h1 id="vi-usr-bin-yum"><a href="#vi-usr-bin-yum" class="headerlink" title="vi /usr/bin/yum"></a>vi /usr/bin/yum</h1><p>将第一句</p>
<h1 id="usr-bin-python-改为-usr-bin-python2-6-即可"><a href="#usr-bin-python-改为-usr-bin-python2-6-即可" class="headerlink" title="!/usr/bin/python 改为 # !/usr/bin/python2.6 即可"></a>!/usr/bin/python 改为 # !/usr/bin/python2.6 即可</h1><p>安装setuptool,后续操作要运行很多py文件，没他不行</p>
<pre><code># wget --no-check-certificate https://pypi.python.org/packages/source/s/setuptools/setuptools-1.4.2.tar.gz
# tar -xvf setuptools-1.4.2.tar.gz
# cd setuptools-1.4.2
# python setup.py install
</code></pre><p>2.升级pip，Python Index Package。类似linux下的yum，安装并管理python软件包，Python升级了版本pip也要升级相应版本，下载pip9.0.1</p>
<h1 id="wget-https-pypi-python-org-packages-11-b6-abcb525026a4be042b486df43905d6893fb04f05aac21c32c638e939e447-pip-9-0-1-tar-gz-md5-35f01da33009719497f01a4ba69d63c9"><a href="#wget-https-pypi-python-org-packages-11-b6-abcb525026a4be042b486df43905d6893fb04f05aac21c32c638e939e447-pip-9-0-1-tar-gz-md5-35f01da33009719497f01a4ba69d63c9" class="headerlink" title="wget https://pypi.python.org/packages/11/b6/abcb525026a4be042b486df43905d6893fb04f05aac21c32c638e939e447/pip-9.0.1.tar.gz# md5=35f01da33009719497f01a4ba69d63c9"></a>wget <a href="https://pypi.python.org/packages/11/b6/abcb525026a4be042b486df43905d6893fb04f05aac21c32c638e939e447/pip-9.0.1.tar.gz#" target="_blank" rel="external">https://pypi.python.org/packages/11/b6/abcb525026a4be042b486df43905d6893fb04f05aac21c32c638e939e447/pip-9.0.1.tar.gz#</a> md5=35f01da33009719497f01a4ba69d63c9</h1><pre><code># tar zxvf pip-9.0.1.tar.gz
#  cd pip-9.0.1
#  python setup.py install
</code></pre><p>3.执行：</p>
<h1 id="pip-install-–upgrade-https-storage-googleapis-com-tensorflow-linux-cpu-tensorflow-0-8-0-cp27-none-linux-x86-64-whl"><a href="#pip-install-–upgrade-https-storage-googleapis-com-tensorflow-linux-cpu-tensorflow-0-8-0-cp27-none-linux-x86-64-whl" class="headerlink" title="pip install –upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl"></a>pip install –upgrade <a href="https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl" target="_blank" rel="external">https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl</a></h1><p>正常情况下pip会自动下载依赖包并安装，但是也有可能会报依赖没有找到的情况，如果出现此情况见附录，在最后可能会报错，在执行一次就好了</p>
<p>4.然后输入命令：</p>
<h1 id="python"><a href="#python" class="headerlink" title="python"></a>python</h1><p>出现 &gt;&gt;&gt; 再输入：<br>improt ssl<br>再输入import tensorflow as tf 会出现/lib64/libc.so.6: version `GLIBC_2.15’ not found 的错误，这是由于tensorflow0.80版本编译的时候使用GLIBC_2.15，系统自带的是GLIBC_2.12，所以报错了。<br>安装gcc</p>
<pre><code># yum install gcc
</code></pre><p>下载c++库glibc</p>
<pre><code># wget http://ftp.gnu.org/pub/gnu/glibc/glibc-2.17.tar.xz
# xz -d glibc-2.17.tar.xz
# tar -xvf glibc-2.17.tar
# cd glibc-2.17
# mkdir build
# cd build
# ../configure --prefix=/usr --disable-profile --enable-add-ons         --with-headers=/usr/include --with-binutils=/usr/bin  
# make &amp;&amp; make install
</code></pre><p>输入strings /lib64/libc.so.6|grep GLIBC发现有我们的2.15版本</p>
<p>5.再执行第5步，发现报/usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.14’ not found的错误，输入命令strings /usr/lib64/libstdc++.so.6 | grep GLIBCXX 发现没有支持3.4.14版本，去csdn下载一个高版本的libstdc++.so.6，比如说libstdc++.so.6.0.20，网址：<a href="http://download.csdn.net/download/arackethis/8395651，将文件放到/usr/lib64/下，在该目录下执行命令：" target="_blank" rel="external">http://download.csdn.net/download/arackethis/8395651，将文件放到/usr/lib64/下，在该目录下执行命令：</a></p>
<pre><code># chmod +x libstdc++.so.6.0.20
# rm libstdc++.so.6
# ln -s libstdc++.so.6.0.20 libstdc++.so.6
# strings /usr/lib64/libstdc++.so.6 | grep GLIBCXX
</code></pre><p>发现支持了3.4.14版本<br> 再执行第五步发现没有报错了，就此TensorFlow搭建成功</p>
<p>Ps:第三步中依赖报错的解决方式<br>1.安装six</p>
<h1 id="wget-https-pypi-python-org-packages-c8-0a-b6723e1bc4c516cb687841499455a8505b446-07ab535-be01091c0f24f079-six-1-10-0-py2-py3-none-any-whl-md5-3ab558cf5d4f7a72611d5-9a81a31-5dc8-下载指令"><a href="#wget-https-pypi-python-org-packages-c8-0a-b6723e1bc4c516cb687841499455a8505b446-07ab535-be01091c0f24f079-six-1-10-0-py2-py3-none-any-whl-md5-3ab558cf5d4f7a72611d5-9a81a31-5dc8-下载指令" class="headerlink" title="wget     https://pypi.python.org/packages/c8/0a/b6723e1bc4c516cb687841499455a8505b446    07ab535    be01091c0f24f079/six-1.10.0-py2.py3-none-any.whl#md5=3ab558cf5d4f7a72611d5        9a81a31    5dc8  #下载指令"></a>wget     <a href="https://pypi.python.org/packages/c8/0a/b6723e1bc4c516cb687841499455a8505b446" target="_blank" rel="external">https://pypi.python.org/packages/c8/0a/b6723e1bc4c516cb687841499455a8505b446</a>    07ab535    be01091c0f24f079/six-1.10.0-py2.py3-none-any.whl#md5=3ab558cf5d4f7a72611d5        9a81a31    5dc8  #下载指令</h1><h1 id="pip-install-six-1-10-0-py2-py3-none-any-whl-安装指令"><a href="#pip-install-six-1-10-0-py2-py3-none-any-whl-安装指令" class="headerlink" title="pip install six-1.10.0-py2.py3-none-any.whl  #安装指令"></a>pip install six-1.10.0-py2.py3-none-any.whl  #安装指令</h1><p>2.安装numpy，下载地址：<a href="http://jaist.dl.sourceforge.net/project/numpy/NumPy/1.11.2rc1/，解压使用python安装" target="_blank" rel="external">http://jaist.dl.sourceforge.net/project/numpy/NumPy/1.11.2rc1/，解压使用python安装</a><br>3.安装protobuf</p>
<p>#wget             <a href="https://pypi.python.org/packages/0f/53/e43b226f83a5a542c16695e9624b7bd2bde4ad016776c" target="_blank" rel="external">https://pypi.python.org/packages/0f/53/e43b226f83a5a542c16695e9624b7bd2bde4ad016776c</a>    7c3233901bcf5b4/protobuf-3.2.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=a1a807fee3a        7df784e171837853cc29d  #下载指令</p>
<p>#[root@bogon tool]# pip install protobuf-3.2.0-cp27-cp27mu-manylinux1_x86_64.whl </p>
<p>这一步可能会用pip安装　提示找不到ssl模块，参考<a href="http://blog.csdn.net/qq_25560423/article/details/62055497" target="_blank" rel="external">http://blog.csdn.net/qq_25560423/article/details/62055497</a></p>
<p>Ps:正文中第二步pip另外的安装方式：pip安装命令：yum install  python-pip python-devel</p>
<p>补充TensorFlow中TensorBorad可视化组件的使用：<br>如何更直观的观察数据在神经网络中的变化，或是已经构建的神经网络的结构，Tensorflow自带了可视化模块Tensorboard，并且能更直观的看见整个神经网络的结构。</p>
<p>TensorBoard的输入是tensorflow保存summary data的日志文件。日志文件名的形式如：events.out.tfevents.1467809796.lei-All-Series 或 events.out.tfevents.1467809800.lei-All-Series。TensorBoard可读的summary data有scalar，images，audio，histogram和graph。那么怎么把这些summary data保存在日志文件中呢？<br>数值如学习率，损失函数用scalar_summary函数。tf.scalar_summary(节点名称，获取的数据)<br>    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))<br>    tf.scalar_summary(‘accuracy’, accuracy)  </p>
<p>各层网络权重，偏置的分布，用histogram_summary函数<br>    preactivate = tf.matmul(input_tensor, weights) + biases<br>    tf.histogram_summary(layer_name + ‘/pre_activations’, preactivate) </p>
<p>其他几种summary data也是同样的方式获取，只是对应的获取函数名称换一下。这些获取summary data函数节点和graph是独立的，调用的时候也需要运行session。当需要获取的数据较多的时候，我们一个一个去保存获取到的数据，以及一个一个去运行会显得比较麻烦。tensorflow提供了一个简单的方法，就是合并所有的summary data的获取函数，保存和运行只对一个对象进行操作。比如，写入默认路径中，比如/tmp/mnist_logs (by default)<br>    merged = tf.merge_all_summaries()<br>    train_writer = tf.train.SummaryWriter(FLAGS.summaries_dir + ‘/train’, sess.graph)<br>    test_writer = tf.train.SummaryWriter(FLAGS.summaries_dir + ‘/test’)  </p>
<p>SummaryWriter从tensorflow获取summary data，然后保存到指定路径的日志文件中。以上是在建立graph的过程中，接下来执行，每隔一定step，写入网络参数到默认路径中，形成最开始的文件：events.out.tfevents.1467809796.lei-All-Series 或 events.out.tfevents.1467809800.lei-All-Series。<br>    for i in range(FLAGS.max_steps):<br>    if i % 10 == 0:  # Record summaries and test-set accuracy<br>    summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))<br>          test_writer.add<em>summary(summary, i)<br>          print(‘Accuracy at step %s: %s’ % (i, acc))<br>        else: # Record train set summarieis, and train<br>          summary, </em> = sess.run([merged, train_step], feed_dict=feed_dict(True))<br>          train_writer.add_summary(summary, i)  </p>
<p>源码：<br>    ‘’’<br>    Created on May 9, 2017</p>
<pre><code>@author: root
&apos;&apos;&apos;
# Copyright 2015 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the &apos;License&apos;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &apos;AS IS&apos; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
&quot;&quot;&quot;A simple MNIST classifier which displays summaries in TensorBoard.
 This is an unimpressive MNIST model, but it is a good example of using
tf.name_scope to make a graph legible in the TensorBoard graph explorer, and of
naming summary tags so that they are grouped meaningfully in TensorBoard.
It demonstrates the functionality of every TensorBoard dashboard.
&quot;&quot;&quot;


import tensorflow as tf

from tensorflow.examples.tutorials.mnist import input_data

flags = tf.app.flags
FLAGS = flags.FLAGS
flags.DEFINE_boolean(&apos;fake_data&apos;, False, &apos;If true, uses fake data &apos;
                     &apos;for unit testing.&apos;)
flags.DEFINE_integer(&apos;max_steps&apos;, 1000, &apos;Number of steps to run trainer.&apos;)
flags.DEFINE_float(&apos;learning_rate&apos;, 0.001, &apos;Initial learning rate.&apos;)
flags.DEFINE_float(&apos;dropout&apos;, 0.9, &apos;Keep probability for training dropout.&apos;)
flags.DEFINE_string(&apos;data_dir&apos;, &apos;/tmp/data&apos;, &apos;Directory for storing data&apos;)
flags.DEFINE_string(&apos;summaries_dir&apos;, &apos;/tmp/mnist_logs&apos;, &apos;Summaries directory&apos;)


def train():
  # Import data
  mnist = input_data.read_data_sets(FLAGS.data_dir,
                                    one_hot=True,
                                    fake_data=FLAGS.fake_data)

  sess = tf.InteractiveSession()

  # Create a multilayer model.

  # Input placehoolders
  with tf.name_scope(&apos;input&apos;):
    x = tf.placeholder(tf.float32, [None, 784], name=&apos;x-input&apos;)
    y_ = tf.placeholder(tf.float32, [None, 10], name=&apos;y-input&apos;)

  with tf.name_scope(&apos;input_reshape&apos;):
    image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])
    tf.image_summary(&apos;input&apos;, image_shaped_input, 10)

  # We can&apos;t initialize these variables to 0 - the network will get stuck.
  def weight_variable(shape):
    &quot;&quot;&quot;Create a weight variable with appropriate initialization.&quot;&quot;&quot;
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)

  def bias_variable(shape):
    &quot;&quot;&quot;Create a bias variable with appropriate initialization.&quot;&quot;&quot;
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)

  def variable_summaries(var, name):
    &quot;&quot;&quot;Attach a lot of summaries to a Tensor.&quot;&quot;&quot;
    with tf.name_scope(&apos;summaries&apos;):
      mean = tf.reduce_mean(var)
      tf.scalar_summary(&apos;mean/&apos; + name, mean)
      with tf.name_scope(&apos;stddev&apos;):
        stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))
      tf.scalar_summary(&apos;sttdev/&apos; + name, stddev)
      tf.scalar_summary(&apos;max/&apos; + name, tf.reduce_max(var))
      tf.scalar_summary(&apos;min/&apos; + name, tf.reduce_min(var))
      tf.histogram_summary(name, var)

  def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):
    &quot;&quot;&quot;Reusable code for making a simple neural net layer.
    It does a matrix multiply, bias add, and then uses relu to nonlinearize.
    It also sets up name scoping so that the resultant graph is easy to read,
    and adds a number of summary ops.
    &quot;&quot;&quot;
    # Adding a name scope ensures logical grouping of the layers in the graph.
    with tf.name_scope(layer_name):
      # This Variable will hold the state of the weights for the layer
      with tf.name_scope(&apos;weights&apos;):
        weights = weight_variable([input_dim, output_dim])
        variable_summaries(weights, layer_name + &apos;/weights&apos;)
      with tf.name_scope(&apos;biases&apos;):
        biases = bias_variable([output_dim])
        variable_summaries(biases, layer_name + &apos;/biases&apos;)
      with tf.name_scope(&apos;Wx_plus_b&apos;):
        preactivate = tf.matmul(input_tensor, weights) + biases
        tf.histogram_summary(layer_name + &apos;/pre_activations&apos;, preactivate)
      activations = act(preactivate, &apos;activation&apos;)
      tf.histogram_summary(layer_name + &apos;/activations&apos;, activations)
      return activations

  hidden1 = nn_layer(x, 784, 500, &apos;layer1&apos;)

  with tf.name_scope(&apos;dropout&apos;):
    keep_prob = tf.placeholder(tf.float32)
    tf.scalar_summary(&apos;dropout_keep_probability&apos;, keep_prob)
    dropped = tf.nn.dropout(hidden1, keep_prob)

  y = nn_layer(dropped, 500, 10, &apos;layer2&apos;, act=tf.nn.softmax)

  with tf.name_scope(&apos;cross_entropy&apos;):
    diff = y_ * tf.log(y)
    with tf.name_scope(&apos;total&apos;):
      cross_entropy = -tf.reduce_mean(diff)
    tf.scalar_summary(&apos;cross entropy&apos;, cross_entropy)

  with tf.name_scope(&apos;train&apos;):
    train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(
        cross_entropy)

  with tf.name_scope(&apos;accuracy&apos;):
    with tf.name_scope(&apos;correct_prediction&apos;):
      correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
    with tf.name_scope(&apos;accuracy&apos;):
      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    tf.scalar_summary(&apos;accuracy&apos;, accuracy)

  # Merge all the summaries and write them out to /tmp/mnist_logs (by default)
  merged = tf.merge_all_summaries()
  train_writer = tf.train.SummaryWriter(FLAGS.summaries_dir + &apos;/train&apos;,
                                        sess.graph)
  test_writer = tf.train.SummaryWriter(FLAGS.summaries_dir + &apos;/test&apos;)
  tf.initialize_all_variables().run()

  # Train the model, and also write summaries.
  # Every 10th step, measure test-set accuracy, and write test summaries
  # All other steps, run train_step on training data, &amp; add training summaries

  def feed_dict(train):
    &quot;&quot;&quot;Make a TensorFlow feed_dict: maps data onto Tensor placeholders.&quot;&quot;&quot;
    if train or FLAGS.fake_data:
      xs, ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)
      k = FLAGS.dropout
    else:
      xs, ys = mnist.test.images, mnist.test.labels
      k = 1.0
    return {x: xs, y_: ys, keep_prob: k}

  for i in range(FLAGS.max_steps):
    if i % 10 == 0:  # Record summaries and test-set accuracy
      summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))
      test_writer.add_summary(summary, i)
      print(&apos;Accuracy at step %s: %s&apos; % (i, acc))
    else:  # Record train set summaries, and train
      if i % 100 == 99:  # Record execution stats
        run_options = tf.RunOptions(trace_level=1)
        run_metadata = tf.RunMetadata()
        summary, _ = sess.run([merged, train_step],
                              feed_dict=feed_dict(True),
                              options=run_options,
                              run_metadata=run_metadata)
        train_writer.add_run_metadata(run_metadata, &apos;step%d&apos; % i)
        train_writer.add_summary(summary, i)
        print(&apos;Adding run metadata for&apos;, i)
      else:  # Record a summary
        summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))
        train_writer.add_summary(summary, i)


def main(_):
  if tf.gfile.Exists(FLAGS.summaries_dir):
    tf.gfile.DeleteRecursively(FLAGS.summaries_dir)
  tf.gfile.MakeDirs(FLAGS.summaries_dir)
  train()


if __name__ == &apos;__main__&apos;:
  tf.app.run()
</code></pre><p>这段源码执行后会在/tmp目录下生成data目录和mnist_logs目录，data是需要分析的数据，mnist_logs是分析的日志，调用TensorBoard可视化运行结果<br>    tensorboard –logdir=/tmp/mnist_logs/train/ </p>
<p>打开链接 <a href="http://127.0.0.0:6006" target="_blank" rel="external">http://127.0.0.0:6006</a> 即可看到效果。参考 <a href="http://blog.csdn.net/helei001/article/details/51842531" target="_blank" rel="external">http://blog.csdn.net/helei001/article/details/51842531</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TensorFlow™ 是一个采用数据流图（data flow graphs），用于数值计算的开源软件库。节点（Nodes）在图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor）。它灵活的架构让你可以在多种平台上展开计算，例如台式计算机中的一个或多个CPU（或GPU），服务器，移动设备等等。TensorFlow 最初由Google大脑小组（隶属于Google机器智能研究机构）的研究员和工程师们开发出来，用于机器学习和深度神经网络方面的研究，但这个系统的通用性使其也可广泛用于其他计算领域。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yzy755.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="python" scheme="http://yzy755.github.io/tags/python/"/>
    
      <category term="AI" scheme="http://yzy755.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow搭建和TensorBoard可视化的使用</title>
    <link href="http://yzy755.github.io/2017/05/09/TensorFlow%E6%90%AD%E5%BB%BA%E5%92%8CTensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>http://yzy755.github.io/2017/05/09/TensorFlow搭建和TensorBoard可视化的使用/</id>
    <published>2017-05-09T10:17:53.000Z</published>
    <updated>2017-05-10T09:19:45.866Z</updated>
    
    <content type="html"><![CDATA[<p>TensorFlow™ 是一个采用数据流图（data flow graphs），用于数值计算的开源软件库。节点（Nodes）在图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor）。它灵活的架构让你可以在多种平台上展开计算，例如台式计算机中的一个或多个CPU（或GPU），服务器，移动设备等等。TensorFlow 最初由Google大脑小组（隶属于Google机器智能研究机构）的研究员和工程师们开发出来，用于机器学习和深度神经网络方面的研究，但这个系统的通用性使其也可广泛用于其他计算领域。<br><a id="more"></a><br>1.Centos6.x默认使用的Python版本为2.6.6，而TensorFlow需要的Python版本起步为2.7以上，首先升级Python版本，已2.7.3为例：</p>
<h1 id="wget-https-www-python-org-ftp-python-2-7-3-Python-2-7-3-tgz"><a href="#wget-https-www-python-org-ftp-python-2-7-3-Python-2-7-3-tgz" class="headerlink" title="wget https://www.python.org/ftp/python/2.7.3/Python-2.7.3.tgz"></a>wget <a href="https://www.python.org/ftp/python/2.7.3/Python-2.7.3.tgz" target="_blank" rel="external">https://www.python.org/ftp/python/2.7.3/Python-2.7.3.tgz</a></h1><h1 id="tar-xvjf-Python-2-7-3-tar-bz2"><a href="#tar-xvjf-Python-2-7-3-tar-bz2" class="headerlink" title="tar -xvjf Python-2.7.3.tar.bz2"></a>tar -xvjf Python-2.7.3.tar.bz2</h1><h1 id="mkdir-usr-local-python27"><a href="#mkdir-usr-local-python27" class="headerlink" title="mkdir /usr/local/python27"></a>mkdir /usr/local/python27</h1><h1 id="configure-–prefix-usr-local-python27"><a href="#configure-–prefix-usr-local-python27" class="headerlink" title="./configure –prefix=/usr/local/python27"></a>./configure –prefix=/usr/local/python27</h1><h1 id="Make"><a href="#Make" class="headerlink" title="Make"></a>Make</h1><h1 id="make-install"><a href="#make-install" class="headerlink" title="make install"></a>make install</h1><p>以上安装好了2.7.3，但此时没有覆盖老版本，再将原来/usr/bin/python链接改为别的名字：</p>
<h1 id="mv-usr-bin-python-usr-bin-python-old"><a href="#mv-usr-bin-python-usr-bin-python-old" class="headerlink" title="mv /usr/bin/python /usr/bin/python_old"></a>mv /usr/bin/python /usr/bin/python_old</h1><p>再建立新版本python的软链接：</p>
<h1 id="ln-s-usr-local-python27-bin-python2-7-usr-bin-python"><a href="#ln-s-usr-local-python27-bin-python2-7-usr-bin-python" class="headerlink" title="ln -s /usr/local/python27/bin/python2.7 /usr/bin/python"></a>ln -s /usr/local/python27/bin/python2.7 /usr/bin/python</h1><p>再输入 </p>
<h1 id="python-–version"><a href="#python-–version" class="headerlink" title="python –version"></a>python –version</h1><p>会出现2.7.3则说明成功</p>
<p>但是，基于Python编写的yum不兼容2.7以上版本，所以修改yum命令指定的Python版本:</p>
<h1 id="vi-usr-bin-yum"><a href="#vi-usr-bin-yum" class="headerlink" title="vi /usr/bin/yum"></a>vi /usr/bin/yum</h1><p>将第一句</p>
<h1 id="usr-bin-python-改为-usr-bin-python2-6-即可"><a href="#usr-bin-python-改为-usr-bin-python2-6-即可" class="headerlink" title="!/usr/bin/python 改为 # !/usr/bin/python2.6 即可"></a>!/usr/bin/python 改为 # !/usr/bin/python2.6 即可</h1><p>安装setuptool,后续操作要运行很多py文件，没他不行</p>
<pre><code># wget --no-check-certificate https://pypi.python.org/packages/source/s/setuptools/setuptools-1.4.2.tar.gz
# tar -xvf setuptools-1.4.2.tar.gz
# cd setuptools-1.4.2
# python setup.py install
</code></pre><p>2.升级pip，Python Index Package。类似linux下的yum，安装并管理python软件包，Python升级了版本pip也要升级相应版本，下载pip9.0.1</p>
<h1 id="wget-https-pypi-python-org-packages-11-b6-abcb525026a4be042b486df43905d6893fb04f05aac21c32c638e939e447-pip-9-0-1-tar-gz-md5-35f01da33009719497f01a4ba69d63c9"><a href="#wget-https-pypi-python-org-packages-11-b6-abcb525026a4be042b486df43905d6893fb04f05aac21c32c638e939e447-pip-9-0-1-tar-gz-md5-35f01da33009719497f01a4ba69d63c9" class="headerlink" title="wget https://pypi.python.org/packages/11/b6/abcb525026a4be042b486df43905d6893fb04f05aac21c32c638e939e447/pip-9.0.1.tar.gz# md5=35f01da33009719497f01a4ba69d63c9"></a>wget <a href="https://pypi.python.org/packages/11/b6/abcb525026a4be042b486df43905d6893fb04f05aac21c32c638e939e447/pip-9.0.1.tar.gz#" target="_blank" rel="external">https://pypi.python.org/packages/11/b6/abcb525026a4be042b486df43905d6893fb04f05aac21c32c638e939e447/pip-9.0.1.tar.gz#</a> md5=35f01da33009719497f01a4ba69d63c9</h1><pre><code># tar zxvf pip-9.0.1.tar.gz
#  cd pip-9.0.1
#  python setup.py install
</code></pre><p>3.执行：</p>
<h1 id="pip-install-–upgrade-https-storage-googleapis-com-tensorflow-linux-cpu-tensorflow-0-8-0-cp27-none-linux-x86-64-whl"><a href="#pip-install-–upgrade-https-storage-googleapis-com-tensorflow-linux-cpu-tensorflow-0-8-0-cp27-none-linux-x86-64-whl" class="headerlink" title="pip install –upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl"></a>pip install –upgrade <a href="https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl" target="_blank" rel="external">https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl</a></h1><p>正常情况下pip会自动下载依赖包并安装，但是也有可能会报依赖没有找到的情况，如果出现此情况见附录，在最后可能会报错，在执行一次就好了</p>
<p>4.然后输入命令：</p>
<h1 id="python"><a href="#python" class="headerlink" title="python"></a>python</h1><p>出现 &gt;&gt;&gt; 再输入：<br>improt ssl<br>再输入import tensorflow as tf 会出现/lib64/libc.so.6: version `GLIBC_2.15’ not found 的错误，这是由于tensorflow0.80版本编译的时候使用GLIBC_2.15，系统自带的是GLIBC_2.12，所以报错了。<br>安装gcc</p>
<pre><code># yum install gcc
</code></pre><p>下载c++库glibc</p>
<pre><code># wget http://ftp.gnu.org/pub/gnu/glibc/glibc-2.17.tar.xz
# xz -d glibc-2.17.tar.xz
# tar -xvf glibc-2.17.tar
# cd glibc-2.17
# mkdir build
# cd build
# ../configure --prefix=/usr --disable-profile --enable-add-ons         --with-headers=/usr/include --with-binutils=/usr/bin  
# make &amp;&amp; make install
</code></pre><p>输入strings /lib64/libc.so.6|grep GLIBC发现有我们的2.15版本</p>
<p>5.再执行第5步，发现报/usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.14’ not found的错误，输入命令strings /usr/lib64/libstdc++.so.6 | grep GLIBCXX 发现没有支持3.4.14版本，去csdn下载一个高版本的libstdc++.so.6，比如说libstdc++.so.6.0.20，网址：<a href="http://download.csdn.net/download/arackethis/8395651，将文件放到/usr/lib64/下，在该目录下执行命令：" target="_blank" rel="external">http://download.csdn.net/download/arackethis/8395651，将文件放到/usr/lib64/下，在该目录下执行命令：</a></p>
<pre><code># chmod +x libstdc++.so.6.0.20
# rm libstdc++.so.6
# ln -s libstdc++.so.6.0.20 libstdc++.so.6
# strings /usr/lib64/libstdc++.so.6 | grep GLIBCXX
</code></pre><p>发现支持了3.4.14版本<br> 再执行第五步发现没有报错了，就此TensorFlow搭建成功</p>
<p>Ps:第三步中依赖报错的解决方式<br>1.安装six</p>
<h1 id="wget-https-pypi-python-org-packages-c8-0a-b6723e1bc4c516cb687841499455a8505b446-07ab535-be01091c0f24f079-six-1-10-0-py2-py3-none-any-whl-md5-3ab558cf5d4f7a72611d5-9a81a31-5dc8-下载指令"><a href="#wget-https-pypi-python-org-packages-c8-0a-b6723e1bc4c516cb687841499455a8505b446-07ab535-be01091c0f24f079-six-1-10-0-py2-py3-none-any-whl-md5-3ab558cf5d4f7a72611d5-9a81a31-5dc8-下载指令" class="headerlink" title="wget     https://pypi.python.org/packages/c8/0a/b6723e1bc4c516cb687841499455a8505b446    07ab535    be01091c0f24f079/six-1.10.0-py2.py3-none-any.whl#md5=3ab558cf5d4f7a72611d5        9a81a31    5dc8  #下载指令"></a>wget     <a href="https://pypi.python.org/packages/c8/0a/b6723e1bc4c516cb687841499455a8505b446" target="_blank" rel="external">https://pypi.python.org/packages/c8/0a/b6723e1bc4c516cb687841499455a8505b446</a>    07ab535    be01091c0f24f079/six-1.10.0-py2.py3-none-any.whl#md5=3ab558cf5d4f7a72611d5        9a81a31    5dc8  #下载指令</h1><h1 id="pip-install-six-1-10-0-py2-py3-none-any-whl-安装指令"><a href="#pip-install-six-1-10-0-py2-py3-none-any-whl-安装指令" class="headerlink" title="pip install six-1.10.0-py2.py3-none-any.whl  #安装指令"></a>pip install six-1.10.0-py2.py3-none-any.whl  #安装指令</h1><p>2.安装numpy，下载地址：<a href="http://jaist.dl.sourceforge.net/project/numpy/NumPy/1.11.2rc1/，解压使用python安装" target="_blank" rel="external">http://jaist.dl.sourceforge.net/project/numpy/NumPy/1.11.2rc1/，解压使用python安装</a><br>3.安装protobuf</p>
<p>#wget             <a href="https://pypi.python.org/packages/0f/53/e43b226f83a5a542c16695e9624b7bd2bde4ad016776c" target="_blank" rel="external">https://pypi.python.org/packages/0f/53/e43b226f83a5a542c16695e9624b7bd2bde4ad016776c</a>    7c3233901bcf5b4/protobuf-3.2.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=a1a807fee3a        7df784e171837853cc29d  #下载指令</p>
<p>#[root@bogon tool]# pip install protobuf-3.2.0-cp27-cp27mu-manylinux1_x86_64.whl </p>
<p>这一步可能会用pip安装　提示找不到ssl模块，参考<a href="http://blog.csdn.net/qq_25560423/article/details/62055497" target="_blank" rel="external">http://blog.csdn.net/qq_25560423/article/details/62055497</a></p>
<p>Ps:正文中第二步pip另外的安装方式：pip安装命令：yum install  python-pip python-devel</p>
<p>补充TensorFlow中TensorBorad可视化组件的使用：<br>如何更直观的观察数据在神经网络中的变化，或是已经构建的神经网络的结构，Tensorflow自带了可视化模块Tensorboard，并且能更直观的看见整个神经网络的结构。</p>
<p>TensorBoard的输入是tensorflow保存summary data的日志文件。日志文件名的形式如：events.out.tfevents.1467809796.lei-All-Series 或 events.out.tfevents.1467809800.lei-All-Series。TensorBoard可读的summary data有scalar，images，audio，histogram和graph。那么怎么把这些summary data保存在日志文件中呢？<br>数值如学习率，损失函数用scalar_summary函数。tf.scalar_summary(节点名称，获取的数据)<br>    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))<br>    tf.scalar_summary(‘accuracy’, accuracy)  </p>
<p>各层网络权重，偏置的分布，用histogram_summary函数<br>    preactivate = tf.matmul(input_tensor, weights) + biases<br>    tf.histogram_summary(layer_name + ‘/pre_activations’, preactivate) </p>
<p>其他几种summary data也是同样的方式获取，只是对应的获取函数名称换一下。这些获取summary data函数节点和graph是独立的，调用的时候也需要运行session。当需要获取的数据较多的时候，我们一个一个去保存获取到的数据，以及一个一个去运行会显得比较麻烦。tensorflow提供了一个简单的方法，就是合并所有的summary data的获取函数，保存和运行只对一个对象进行操作。比如，写入默认路径中，比如/tmp/mnist_logs (by default)<br>    merged = tf.merge_all_summaries()<br>    train_writer = tf.train.SummaryWriter(FLAGS.summaries_dir + ‘/train’, sess.graph)<br>    test_writer = tf.train.SummaryWriter(FLAGS.summaries_dir + ‘/test’)  </p>
<p>SummaryWriter从tensorflow获取summary data，然后保存到指定路径的日志文件中。以上是在建立graph的过程中，接下来执行，每隔一定step，写入网络参数到默认路径中，形成最开始的文件：events.out.tfevents.1467809796.lei-All-Series 或 events.out.tfevents.1467809800.lei-All-Series。<br>    for i in range(FLAGS.max_steps):<br>    if i % 10 == 0:  # Record summaries and test-set accuracy<br>    summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))<br>          test_writer.add<em>summary(summary, i)<br>          print(‘Accuracy at step %s: %s’ % (i, acc))<br>        else: # Record train set summarieis, and train<br>          summary, </em> = sess.run([merged, train_step], feed_dict=feed_dict(True))<br>          train_writer.add_summary(summary, i)  </p>
<p>源码：<br>    ‘’’<br>    Created on May 9, 2017</p>
<pre><code>@author: root
&apos;&apos;&apos;
# Copyright 2015 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the &apos;License&apos;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &apos;AS IS&apos; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
&quot;&quot;&quot;A simple MNIST classifier which displays summaries in TensorBoard.
 This is an unimpressive MNIST model, but it is a good example of using
tf.name_scope to make a graph legible in the TensorBoard graph explorer, and of
naming summary tags so that they are grouped meaningfully in TensorBoard.
It demonstrates the functionality of every TensorBoard dashboard.
&quot;&quot;&quot;


import tensorflow as tf

from tensorflow.examples.tutorials.mnist import input_data

flags = tf.app.flags
FLAGS = flags.FLAGS
flags.DEFINE_boolean(&apos;fake_data&apos;, False, &apos;If true, uses fake data &apos;
                     &apos;for unit testing.&apos;)
flags.DEFINE_integer(&apos;max_steps&apos;, 1000, &apos;Number of steps to run trainer.&apos;)
flags.DEFINE_float(&apos;learning_rate&apos;, 0.001, &apos;Initial learning rate.&apos;)
flags.DEFINE_float(&apos;dropout&apos;, 0.9, &apos;Keep probability for training dropout.&apos;)
flags.DEFINE_string(&apos;data_dir&apos;, &apos;/tmp/data&apos;, &apos;Directory for storing data&apos;)
flags.DEFINE_string(&apos;summaries_dir&apos;, &apos;/tmp/mnist_logs&apos;, &apos;Summaries directory&apos;)


def train():
  # Import data
  mnist = input_data.read_data_sets(FLAGS.data_dir,
                                    one_hot=True,
                                    fake_data=FLAGS.fake_data)

  sess = tf.InteractiveSession()

  # Create a multilayer model.

  # Input placehoolders
  with tf.name_scope(&apos;input&apos;):
    x = tf.placeholder(tf.float32, [None, 784], name=&apos;x-input&apos;)
    y_ = tf.placeholder(tf.float32, [None, 10], name=&apos;y-input&apos;)

  with tf.name_scope(&apos;input_reshape&apos;):
    image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])
    tf.image_summary(&apos;input&apos;, image_shaped_input, 10)

  # We can&apos;t initialize these variables to 0 - the network will get stuck.
  def weight_variable(shape):
    &quot;&quot;&quot;Create a weight variable with appropriate initialization.&quot;&quot;&quot;
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)

  def bias_variable(shape):
    &quot;&quot;&quot;Create a bias variable with appropriate initialization.&quot;&quot;&quot;
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)

  def variable_summaries(var, name):
    &quot;&quot;&quot;Attach a lot of summaries to a Tensor.&quot;&quot;&quot;
    with tf.name_scope(&apos;summaries&apos;):
      mean = tf.reduce_mean(var)
      tf.scalar_summary(&apos;mean/&apos; + name, mean)
      with tf.name_scope(&apos;stddev&apos;):
        stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))
      tf.scalar_summary(&apos;sttdev/&apos; + name, stddev)
      tf.scalar_summary(&apos;max/&apos; + name, tf.reduce_max(var))
      tf.scalar_summary(&apos;min/&apos; + name, tf.reduce_min(var))
      tf.histogram_summary(name, var)

  def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):
    &quot;&quot;&quot;Reusable code for making a simple neural net layer.
    It does a matrix multiply, bias add, and then uses relu to nonlinearize.
    It also sets up name scoping so that the resultant graph is easy to read,
    and adds a number of summary ops.
    &quot;&quot;&quot;
    # Adding a name scope ensures logical grouping of the layers in the graph.
    with tf.name_scope(layer_name):
      # This Variable will hold the state of the weights for the layer
      with tf.name_scope(&apos;weights&apos;):
        weights = weight_variable([input_dim, output_dim])
        variable_summaries(weights, layer_name + &apos;/weights&apos;)
      with tf.name_scope(&apos;biases&apos;):
        biases = bias_variable([output_dim])
        variable_summaries(biases, layer_name + &apos;/biases&apos;)
      with tf.name_scope(&apos;Wx_plus_b&apos;):
        preactivate = tf.matmul(input_tensor, weights) + biases
        tf.histogram_summary(layer_name + &apos;/pre_activations&apos;, preactivate)
      activations = act(preactivate, &apos;activation&apos;)
      tf.histogram_summary(layer_name + &apos;/activations&apos;, activations)
      return activations

  hidden1 = nn_layer(x, 784, 500, &apos;layer1&apos;)

  with tf.name_scope(&apos;dropout&apos;):
    keep_prob = tf.placeholder(tf.float32)
    tf.scalar_summary(&apos;dropout_keep_probability&apos;, keep_prob)
    dropped = tf.nn.dropout(hidden1, keep_prob)

  y = nn_layer(dropped, 500, 10, &apos;layer2&apos;, act=tf.nn.softmax)

  with tf.name_scope(&apos;cross_entropy&apos;):
    diff = y_ * tf.log(y)
    with tf.name_scope(&apos;total&apos;):
      cross_entropy = -tf.reduce_mean(diff)
    tf.scalar_summary(&apos;cross entropy&apos;, cross_entropy)

  with tf.name_scope(&apos;train&apos;):
    train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(
        cross_entropy)

  with tf.name_scope(&apos;accuracy&apos;):
    with tf.name_scope(&apos;correct_prediction&apos;):
      correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
    with tf.name_scope(&apos;accuracy&apos;):
      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    tf.scalar_summary(&apos;accuracy&apos;, accuracy)

  # Merge all the summaries and write them out to /tmp/mnist_logs (by default)
  merged = tf.merge_all_summaries()
  train_writer = tf.train.SummaryWriter(FLAGS.summaries_dir + &apos;/train&apos;,
                                        sess.graph)
  test_writer = tf.train.SummaryWriter(FLAGS.summaries_dir + &apos;/test&apos;)
  tf.initialize_all_variables().run()

  # Train the model, and also write summaries.
  # Every 10th step, measure test-set accuracy, and write test summaries
  # All other steps, run train_step on training data, &amp; add training summaries

  def feed_dict(train):
    &quot;&quot;&quot;Make a TensorFlow feed_dict: maps data onto Tensor placeholders.&quot;&quot;&quot;
    if train or FLAGS.fake_data:
      xs, ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)
      k = FLAGS.dropout
    else:
      xs, ys = mnist.test.images, mnist.test.labels
      k = 1.0
    return {x: xs, y_: ys, keep_prob: k}

  for i in range(FLAGS.max_steps):
    if i % 10 == 0:  # Record summaries and test-set accuracy
      summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))
      test_writer.add_summary(summary, i)
      print(&apos;Accuracy at step %s: %s&apos; % (i, acc))
    else:  # Record train set summaries, and train
      if i % 100 == 99:  # Record execution stats
        run_options = tf.RunOptions(trace_level=1)
        run_metadata = tf.RunMetadata()
        summary, _ = sess.run([merged, train_step],
                              feed_dict=feed_dict(True),
                              options=run_options,
                              run_metadata=run_metadata)
        train_writer.add_run_metadata(run_metadata, &apos;step%d&apos; % i)
        train_writer.add_summary(summary, i)
        print(&apos;Adding run metadata for&apos;, i)
      else:  # Record a summary
        summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))
        train_writer.add_summary(summary, i)


def main(_):
  if tf.gfile.Exists(FLAGS.summaries_dir):
    tf.gfile.DeleteRecursively(FLAGS.summaries_dir)
  tf.gfile.MakeDirs(FLAGS.summaries_dir)
  train()


if __name__ == &apos;__main__&apos;:
  tf.app.run()
</code></pre><p>这段源码执行后会在/tmp目录下生成data目录和mnist_logs目录，data是需要分析的数据，mnist_logs是分析的日志，调用TensorBoard可视化运行结果<br>    tensorboard –logdir=/tmp/mnist_logs/train/ </p>
<p>打开链接 <a href="http://127.0.0.0:6006即可看到效果。参考" target="_blank" rel="external">http://127.0.0.0:6006即可看到效果。参考</a> <a href="http://blog.csdn.net/helei001/article/details/51842531" target="_blank" rel="external">http://blog.csdn.net/helei001/article/details/51842531</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TensorFlow™ 是一个采用数据流图（data flow graphs），用于数值计算的开源软件库。节点（Nodes）在图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor）。它灵活的架构让你可以在多种平台上展开计算，例如台式计算机中的一个或多个CPU（或GPU），服务器，移动设备等等。TensorFlow 最初由Google大脑小组（隶属于Google机器智能研究机构）的研究员和工程师们开发出来，用于机器学习和深度神经网络方面的研究，但这个系统的通用性使其也可广泛用于其他计算领域。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yzy755.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="python" scheme="http://yzy755.github.io/tags/python/"/>
    
      <category term="AI" scheme="http://yzy755.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>superset搭建和简单使用</title>
    <link href="http://yzy755.github.io/2017/05/09/superset%E6%90%AD%E5%BB%BA%E5%92%8C%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/"/>
    <id>http://yzy755.github.io/2017/05/09/superset搭建和简单使用/</id>
    <published>2017-05-09T09:25:53.000Z</published>
    <updated>2017-05-10T09:25:15.697Z</updated>
    
    <content type="html"><![CDATA[<p>Superset 是 Airbnb （知名在线房屋短租公司）开源的数据探查与可视化平台（曾用名 Panoramix、Caravel ），该工具在可视化、易用性和交互性上非常有特色，用户可以轻松对数据进行可视化分析。<br><a id="more"></a><br>可先直接执行第二步<br>1./usr/local/python27/bin/pip install –upgrade virtualenv<br>pip install –upgrade distribute<br>virtualenv myenv –distribute</p>
<p>2./usr/local/python27/bin/pip install –upgrade superset<br>提示command ‘gcc’ failed with exit status 1的错误</p>
<pre><code># yum install gcc-c++
# yum install postgresql-devel
# yum -y install gcc gcc-c++ kernel-devel
# yum -y install python-devel libxslt-devel libffi-devel openssl-devel
# yum install openssl-devel python-devel python-sphinx
</code></pre><p>3.安装MySQL-python-1.2.3.tar.gz<br>安装过程中会出现mysql config not found</p>
<pre><code># yum install mysql-devel
</code></pre><p>执行find / -name mysql_config在/usr/bin下就出现了这个配置文件，将这个文件复制到MySQL-python-1.2.3解压目录中，再安装即可,报错就执行</p>
<pre><code># yum install gcc-c++ python-devel.x86_64 cyrus-sasl-devel.x86_64
# pip install thrift_sasl
# pip install sasl
</code></pre><p>再安装即可</p>
<p>4.成功之后将Python的bin目录下的fabmanager、superset、gunicorn文件放到/usr/bin目录下或者建立软连接</p>
<pre><code># fabmanager create-admin --app superset
</code></pre><p>报错No module named _sqlite3或者No module named pysqlite2或者其他，需要安装sqlite-devel之后，<br>执行yum install sqlite-devel，重新编译安装Python即可。</p>
<pre><code># superset db upgrade

# superset load_examples

# superset init

# superset runserver -p 8088 -a 0.0.0.0
</code></pre><p>会报bash: gunicorn: command not found，执行find / -name gunicorn 查找一下这个命令放哪儿了，然后将它放到/usr/bin中去</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Superset 是 Airbnb （知名在线房屋短租公司）开源的数据探查与可视化平台（曾用名 Panoramix、Caravel ），该工具在可视化、易用性和交互性上非常有特色，用户可以轻松对数据进行可视化分析。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yzy755.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="python" scheme="http://yzy755.github.io/tags/python/"/>
    
      <category term="数据库分析" scheme="http://yzy755.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>使用开源软件快速搭建数据分析平台</title>
    <link href="http://yzy755.github.io/2017/05/02/%E4%BD%BF%E7%94%A8%E5%BC%80%E6%BA%90%E8%BD%AF%E4%BB%B6%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0/"/>
    <id>http://yzy755.github.io/2017/05/02/使用开源软件快速搭建数据分析平台/</id>
    <published>2017-05-02T08:38:20.000Z</published>
    <updated>2017-05-03T03:22:07.958Z</updated>
    
    <content type="html"><![CDATA[<p>部门成立了大数据项目组，技术选型已经由其项目组长完成，但是后台开发人员不足，请求我帮助搭建一个大数据分析平台</p>
<a id="more"></a>
<p>整篇文章都是来源于<a href="https://my.oschina.net/taogang/blog/630632" target="_blank" rel="external">https://my.oschina.net/taogang/blog/630632</a> ，感谢伟大的开源精神，也就是按照文章介绍的搭建步骤做一点总结，填了两个小坑</p>
<p>1.安装nodejs<br>2.安装git<br>3.安装bower，命令：npm install -g bower<br>4.安装Python环境，anaconda里面集成了很多关于python科学计算的第三方库，安装方便。下载地址：<a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/" target="_blank" rel="external">https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/</a><br>5.进入源码解压后的目录，进入package\static文件夹，创建名为bower_components的文件夹<br>6.打开cmd使用babel进行jsf的编译，依次执行命令：<br>    a)npm install -g babel-cli<br>    b)npm install babel-preset-es2015 –save<br>    c)npm install babel-preset-react –save<br>    d)babel –presets es2015,react –watch js/ –out-dir lib/<br>7.使用bower安装客户端的所有依赖,命令如下：<br>    a)bower install<br>8.回到package文件夹，执行命令：python main.py</p>
<p>Ps:拓展时若需要更多的开源js组件，可以打开bower.json，在dependency下增加相应的组件名和版本号，然后执行bower install即可。</p>
<p>为以后的大数据方向积累一丁点- -！</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;部门成立了大数据项目组，技术选型已经由其项目组长完成，但是后台开发人员不足，请求我帮助搭建一个大数据分析平台&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yzy755.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="数据分析" scheme="http://yzy755.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
      <category term="python" scheme="http://yzy755.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>抢购秒杀功能的并发设计</title>
    <link href="http://yzy755.github.io/2017/04/20/%E6%8A%A2%E8%B4%AD%E7%A7%92%E6%9D%80%E5%8A%9F%E8%83%BD%E7%9A%84%E5%B9%B6%E5%8F%91%E8%AE%BE%E8%AE%A1/"/>
    <id>http://yzy755.github.io/2017/04/20/抢购秒杀功能的并发设计/</id>
    <published>2017-04-20T06:54:36.000Z</published>
    <updated>2017-04-20T07:45:26.167Z</updated>
    
    <content type="html"><![CDATA[<p>最近云课堂系统出现了购买商品出现并发数据错乱的问题，这是类似电商网站所不能接受的现象，果断重构该模块的业务代码刻不容缓- -！<br><a id="more"></a><br>其实说到底就是多线程并发请求的数据一致性的问题，当然也有很多处理方法，最先想到的肯定是锁（synchronized），可以明确的是加锁是肯定可以解决该问题的，但是所谓互联网项目，若有朝一日并发请求量有的突破，这种线程阻塞的访问方式肯定是体验糟糕的，所以我想试试其他方式。</p>
<p>首先想到的就是基于单线程，多路复用io（mutiplexing）模式的redis，结合我们的业务是关于众筹抢购的，用户占用众筹名额的记录用redis记录非常适用：<br>redis提供了 INCR key 方法，下面是该方法官方的解释：<br>    对存储在指定key的数值执行原子的加1操作。<br>    如果指定的key不存在，那么在执行incr操作之前，会先将它的值设定为0。<br>    如果指定的key中存储的值不是字符串类型（fix：）或者存储的字符串类型不能表示为一个整数，<br>    那么执行这个命令时服务器会返回一个错误(eq:(error) ERR value is not an integer or out of range)。<br>    这个操作仅限于64位的有符号整型数据。<br>注意看，是原子的加1操作，所以记录众筹的数量后，不管并发多少用户抢购众筹，众筹数都是原子加1，不会出现少加从而出现超卖的情况，加上我们业务控制到达限额数量后停止购买功能。<br>有个问题是用户抢占了名额后迟迟不付款，所以我利用了原先就有的检查未付款订单过期的任务，在检查到未支付的众筹订单过期后使用 DECRBY key decrement 做减操作，该方法官方的解释：<br>    将key对应的数字减decrement。如果key不存在，操作之前，key就会被置为0。如果key的value类型错误或者是个不能表示成数字的字符串，就返回错误。这个操作最多支持64位有符号的正型数字。<br>当热，值的添加和删除肯定也是要用到的，参考官方文档 <a href="http://www.redis.cn/" target="_blank" rel="external">http://www.redis.cn/</a><br>这样就释放的未支付抢占的名额，同样这是原子操作。这样整体就保持了高效和线程安全。</p>
<p>一些想法（一些摘抄- -）：<br>    1.Redis为什么使用单进程单线程方式也这么快？<br>        Redis采用的是基于内存的采用的是单进程单线程模型的KV数据库，由C语言编写。官方提供的数据是可以达到100000+的qps。这个数据不比采用单进程多线程的同样基于内存的KV数据库Memcached差。<br>        Redis快的主要原因是：</p>
<pre><code>        完全基于内存
        数据结构简单，对数据操作也简单
        使用多路 I/O 复用模型

        多路 I/O 复用模型是利用select、poll、epoll可以同时监察多个流的 I/O 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有I/O事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll是只轮询那些真正发出了事件的流），并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络IO的时间消耗），且Redis在内存中操作数据的速度非常快（内存内的操作不会成为这里的性能瓶颈），主要以上两点造就了Redis具有很高的吞吐量。
2.volatile 容易掉的坑
    在众筹功能上，占用的众筹数是一个实体变量，我就想volatile关键字不是说是轻量级的synchronized吗，想直接用volatile修饰，其实不然，与 synchronized 块相比，volatile 变量所需的编码较少，并且运行时开销也较少，但是它所能实现的功能也仅是 synchronized的一部分

    锁提供了两种主要特性：互斥（mutual exclusion） 和可见性（visibility）。互斥即一次只允许一个线程持有某个特定的锁，因此可使用该特性实现对共享数据的协调访问协议，这样，一次就只有一个线程能够使用该共享数据。可见性要更加复杂一些，它必须确保释放锁之前对共享数据做出的更改对于随后获得该锁的另一个线程是可见的 —— 如果没有同步机制提供的这种可见性保证，线程看到的共享变量可能是修改前的值或不一致的值，这将引发许多严重问题

    Volatile 变量具有 synchronized 的可见性特性，但是不具备原子特性。这就是说线程能够自动发现 volatile 变量的最新值。Volatile 变量可用于提供线程安全，但是只能应用于非常有限的一组用例：多个变量之间或者某个变量的当前值与修改后值之间没有约束。因此，单独使用 volatile 还不足以实现计数器、互斥锁或任何具有与多个变量相关的不变式（Invariants）的类（例如 “start &lt;=end”）。

    出于简易性或可伸缩性的考虑，您可能倾向于使用 volatile 变量而不是锁。当使用 volatile 变量而非锁时，某些习惯用法（idiom）更加易于编码和阅读。此外，volatile 变量不会像锁那样造成线程阻塞，因此也很少造成可伸缩性问题。在某些情况下，如果读操作远远大于写操作，volatile 变量还可以提供优于锁的性能优势。

    要使 volatile 变量提供理想的线程安全，必须同时满足下面两个条件：
    1.对变量的写操作不依赖于当前值。
    2.该变量没有包含在具有其他变量的不变式中。

    实际上，这些条件表明，可以被写入 volatile 变量的这些有效值独立于任何程序的状态，包括变量的当前状态。
    第一个条件的限制使 volatile 变量不能用作线程安全计数器。虽然增量操作（x++）看上去类似一个单独操作，实际上它是一个由读取－修改－写入操作序列组成的组合操作，必须以原子方式执行，而 volatile 不能提供必须的原子特性。实现正确的操作需要使 x 的值在操作期间保持不变，而 volatile 变量无法实现这点。（然而，如果将值调整为只从单个线程写入，那么可以忽略第一个条件。）
    大多数编程情形都会与这两个条件的其中之一冲突，使得 volatile 变量不能像 synchronized 那样普遍适用于实现线程安全。清单 1 显示了一个非线程安全的数值范围类。它包含了一个不变式 —— 下界总是小于或等于上界。

    所以，还是离volatile远点吧，要始终牢记使用 volatile 的限制 —— 只有在状态真正独立于程序内其他内容时才能使用 volatile。

    贴一段我的测试代码

    @Test
    public void setSynchronizedValue() {
        final Jedis superjedis = getJedis();
        superjedis.set(&quot;age&quot;, &quot;1&quot;);
        for (int i = 0; i &lt; 50; i++) {
            new Thread(new Runnable() {
                Jedis jedis = getJedis();
                @Override
                public void run() {
                    System.out.println(jedis.incr(&quot;age&quot;));
                }
            }).start();
        }

    }

    @Test
    public void setNormalValue() {
        for (int i = 0; i &lt; 50; i++) {
            new Thread(new Runnable() {
                @Override
                public void run() {
                    // TODO Auto-generated method stub
                    int currAge = age; // age 使用volatile static全局定义
                    System.out.println(++ currAge);
                    testInt = currAge;
                }
            }).start();
        }
    }

    运行后可以看出，程序运行的结果是不确定的，这说明了++ currAge并不是原子级别的操作。

    原因是声明为volatile的变量若与自身相关，如以下的声明方式：n=n+1,n++等，那么声明为volatile的变量就不起作用，也就是说关键字volatile无效。
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近云课堂系统出现了购买商品出现并发数据错乱的问题，这是类似电商网站所不能接受的现象，果断重构该模块的业务代码刻不容缓- -！&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yzy755.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="redis" scheme="http://yzy755.github.io/tags/redis/"/>
    
      <category term="线程安全" scheme="http://yzy755.github.io/tags/%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>redis启动错误</title>
    <link href="http://yzy755.github.io/2017/04/18/redis%E5%90%AF%E5%8A%A8%E9%94%99%E8%AF%AF/"/>
    <id>http://yzy755.github.io/2017/04/18/redis启动错误/</id>
    <published>2017-04-18T03:11:36.000Z</published>
    <updated>2017-04-18T03:36:59.795Z</updated>
    
    <content type="html"><![CDATA[<p>Creating Server TCP listening socket *:6379: bind:Unknown error<br><a id="more"></a><br>今天遇到的问题，摘抄下网上的解决方案<br>按顺序输入如下命令就可以连接成功</p>
<ol>
<li>redis-cli.exe</li>
<li>shutdown</li>
<li>exit</li>
<li>redis-server.exe redis.windows.conf</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Creating Server TCP listening socket *:6379: bind:Unknown error&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yzy755.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="redis" scheme="http://yzy755.github.io/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>第三方快捷登录开发后对oauth的认识</title>
    <link href="http://yzy755.github.io/2017/04/13/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BF%AB%E6%8D%B7%E7%99%BB%E5%BD%95%E5%BC%80%E5%8F%91%E5%90%8E%E5%AF%B9oauth%E7%9A%84%E8%AE%A4%E8%AF%86/"/>
    <id>http://yzy755.github.io/2017/04/13/第三方快捷登录开发后对oauth的认识/</id>
    <published>2017-04-13T06:17:53.000Z</published>
    <updated>2017-04-13T03:56:09.779Z</updated>
    
    <content type="html"><![CDATA[<p>公司用户中心系统不仅需要传统的注册登录，主流的快捷登录肯定也必不可少，第三方快捷登录的流程和相应系统数据表的设计就先不记录了，主要摘抄一下网上对oauth的介绍和自己的理解<br><a id="more"></a><br>OAuth是一个关于授权（authorization）的开放网络标准—-&gt;open authorization </p>
<p>应用场景：</p>
<p>有一个”云冲印”的网站，可以将用户储存在Google的照片，冲印出来。用户为了使用该服务，必须让”云冲印”读取自己储存在Google上的照片。</p>
<p>问题是只有得到用户的授权，Google才会同意”云冲印”读取这些照片。那么，”云冲印”怎样获得用户的授权呢？<br>传统方法是，用户将自己的Google用户名和密码，告诉”云冲印”，后者就可以读取用户的照片了。这样的做法有以下几个严重的缺点。<br>（1）”云冲印”为了后续的服务，会保存用户的密码，这样很不安全。<br>（2）Google不得不部署密码登录，而我们知道，单纯的密码登录并不安全。<br>（3）”云冲印”拥有了获取用户储存在Google所有资料的权力，用户没法限制”云冲印”获得授权的范围和有效期。<br>（4）用户只有修改密码，才能收回赋予”云冲印”的权力。但是这样做，会使得其他所有获得用户授权的第三方应用程序全部失效。<br>（5）只要有一个第三方应用程序被破解，就会导致用户密码泄漏，以及所有被密码保护的数据泄漏。<br>OAuth就是为了解决上面这些问题而诞生的。<br>原理：<br>OAuth在”客户端”与”服务提供商”之间，设置了一个授权层（authorization layer）。”客户端”不能直接登录”服务提供商”，只能登录授权层，以此将用户与客户端区分开来。”客户端”登录授权层所用的令牌（token），与用户的密码不同。用户可以在登录的时候，指定授权层令牌的权限范围和有效期。<br>“客户端”登录授权层以后，”服务提供商”根据令牌的权限范围和有效期，向”客户端”开放用户储存的资料。</p>
<p>客户端的授权模式<br>客户端必须得到用户的授权（authorization grant），才能获得令牌（access token）。OAuth 2.0定义了四种授权方式。<br>授权码模式（authorization code）<br>简化模式（implicit）<br>密码模式（resource owner password credentials）<br>客户端模式（client credentials）</p>
<p>授权码模式<br>授权码模式（authorization code）是功能最完整、流程最严密的授权模式。它的特点就是通过客户端的后台服务器，与”服务提供商”的认证服务器进行互动。令牌对访问者是不可见的，后台是由httpclient获取令牌和所需信息的。<br><img src="/2017/04/13/第三方快捷登录开发后对oauth的认识/oauth1.png" alt="logo"></p>
<p>（A）用户访问客户端，后者将前者导向认证服务器。<br>（B）用户选择是否给予客户端授权。<br>（C）假设用户给予授权，认证服务器将用户导向客户端事先指定的”重定向URI”（redirection URI），同时附上一个授权码。<br>（D）客户端收到授权码，附上早先的”重定向URI”，向认证服务器申请令牌。这一步是在客户端的后台的服务器上完成的，对用户不可见。<br>（E）认证服务器核对了授权码和重定向URI，确认无误后，向客户端发送访问令牌（access token）和更新令牌（refresh token）。<br>……</p>
<p>A步骤中，客户端申请认证的URI，包含以下参数：<br>response_type：表示授权类型，必选项，此处的值固定为”code”<br>client_id：表示客户端的ID，必选项<br>redirect_uri：表示重定向URI，必选项<br>scope：表示申请的权限范围，可选项<br>state：表示客户端的当前状态，可以指定任意值，认证服务器会原封不动地返回这个值。<br>示例url：<a href="http://openapi.qzone.qq.com/oauth/show?which=Login&amp;display=pc&amp;response_type=code&amp;client_id=101188868&amp;redirect_uri=account.booway.com.cn&amp;state=qqdenglu__http://yun.booway.com.cn/index.xhtml&amp;scope=get_user_info,get_info,add_t%20del_t%20add_pic_t,get_repost_list,get_other_info%20get_fanslist,get_idollist%20add_idol%20del_idol" target="_blank" rel="external">http://openapi.qzone.qq.com/oauth/show?which=Login&amp;display=pc&amp;response_type=code&amp;client_id=101188868&amp;redirect_uri=account.booway.com.cn&amp;state=qqdenglu__http://yun.booway.com.cn/index.xhtml&amp;scope=get_user_info,get_info,add_t%20del_t%20add_pic_t,get_repost_list,get_other_info%20get_fanslist,get_idollist%20add_idol%20del_idol</a><br>C步骤中，服务器回应客户端的URI，包含以下参数：<br>code：表示授权码，必选项。该码的有效期应该很短，通常设为10分钟，客户端只能使用该码一次，否则会被授权服务器拒绝。该码与客户端ID和重定向URI，是一一对应关系。<br>state：如果客户端的请求中包含这个参数，认证服务器的回应也必须一模一样包含这个参数。<br>D步骤中，客户端向认证服务器申请令牌的HTTP请求，包含以下参数：<br>grant_type：表示使用的授权模式，必选项，此处的值固定为”authorization_code”。<br>code：表示上一步获得的授权码，必选项。<br>redirect_uri：表示重定向URI，必选项，且必须与A步骤中的该参数值保持一致。<br>client_id：表示客户端ID，必选项。<br>E步骤中，认证服务器发送的HTTP回复，包含以下参数：<br>access_token：表示访问令牌，必选项。<br>token_type：表示令牌类型，该值大小写不敏感，必选项，可以是bearer类型或mac类型。<br>expires_in：表示过期时间，单位为秒。如果省略该参数，必须其他方式设置过期时间。<br>refresh_token：表示更新令牌，用来获取下一次的访问令牌，可选项。<br>scope：表示权限范围，如果与客户端申请的范围一致，此项可省略。<br><img src="/2017/04/13/第三方快捷登录开发后对oauth的认识/oauth2.png" alt="logo"><br>云计算应用实例：QQ第三方登录</p>
<p>简化模式<br>简化模式（implicit grant type）不通过第三方应用程序的服务器，直接在浏览器中向认证服务器申请令牌，跳过了”授权码”这个步骤，因此得名。所有步骤在浏览器中完成，令牌对访问者是可见的，且客户端不需要认证。<br><img src="/2017/04/13/第三方快捷登录开发后对oauth的认识/oauth3.png" alt="logo"></p>
<p>步骤如下：<br>（A）客户端将用户导向认证服务器。<br>（B）用户决定是否给于客户端授权。<br>（C）假设用户给予授权，认证服务器将用户导向客户端指定的”重定向URI”，并在URI的Hash部分包含了访问令牌。<br>（D）浏览器向资源服务器发出请求，其中不包括上一步收到的Hash值。<br>（E）资源服务器返回一个网页，其中包含的代码可以获取Hash值中的令牌。<br>（F）浏览器执行上一步获得的脚本，提取出令牌。<br>（G）浏览器将令牌发给客户端。<br>A步骤中，客户端发出的HTTP请求，包含以下参数：<br>response_type：表示授权类型，此处的值固定为”token”，必选项。<br>client_id：表示客户端的ID，必选项。<br>redirect_uri：表示重定向的URI，必选项。<br>scope：表示权限范围，可选项。<br>state：表示客户端的当前状态，可以指定任意值，认证服务器会原封不动地返回这个值。<br>示例url：localhost:8080/booway_uums/openapi/oauth2/authorize-implicit?client_id=123&amp;response_type=token&amp;redirect_uri=oob<br>C步骤中，认证服务器回应客户端的URI，包含以下参数：<br>access_token：表示访问令牌，必选项。<br>token_type：表示令牌类型，该值大小写不敏感，必选项。可以是bearer类型或mac类型。<br>expires_in：表示过期时间，单位为秒。如果省略该参数，必须其他方式设置过期时间。<br>scope：表示权限范围，如果与客户端申请的范围一致，此项可省略。<br>state：如果客户端的请求中包含这个参数，认证服务器的回应也必须一模一样包含这个参数。<br>示例url：<a href="http://localhost:8080/booway_uums/index/success.html#access_token=cb1617c64f1d6c2937c64b3599f2dd2&amp;state=null&amp;token_type=bearer&amp;expires_in=604800000&amp;scope=readCalendar" target="_blank" rel="external">http://localhost:8080/booway_uums/index/success.html#access_token=cb1617c64f1d6c2937c64b3599f2dd2&amp;state=null&amp;token_type=bearer&amp;expires_in=604800000&amp;scope=readCalendar</a><br><img src="/2017/04/13/第三方快捷登录开发后对oauth的认识/oauth4.png" alt="logo"></p>
<p>Access Token 类型介绍<br>OAuth2.0支持Bearer和MAC两种类型的Access Token。其中MAC类型只适用于API2.0接口：<br>Bearer 介绍<br>优点：<br>调用简单，不需要对请求进行签名。<br>缺点：<br>请求API需要使用https协议保证信息传输安全。<br>Access Token有效期一个月，过期后需要使用Refresh Token进行刷新。<br>MAC 介绍<br>优点：<br>不依赖https协议，无协议加密带来的性能开销。<br>Access Token长期有效，无需使用Refresh Token刷新。<br>缺点：<br>需要进行MAC计算。<br>默认情况下，会获得Bearer类型的Access Token。如果开发者想要获得MAC类型的Access Token，需要在获取token时指定’token_type’参数为’mac’。</p>
<p>MAC计算<br>标准化字符串<br>标准化的请求字符串，就是用指定的请求属性按照某个规则拼接而成的字符串。在这里，其实标准化请求字符串，就是将 时间戳 + 随机码 + http方法 + uri + 主机 + 端口 + 其他参数的请求参数，以换行符(即：\n)为连接符，合并起来得到的字符串。即使不需要其他参数也请在最后增加\n。</p>
<p>如果用户访问的时候，客户端的”访问令牌”已经过期，则需要使用”更新令牌”申请一个新的访问令牌。<br>客户端发出更新令牌的HTTP请求，包含以下参数：<br>granttype：表示使用的授权模式，此处的值固定为”refreshtoken”，必选项。<br>refresh_token：表示早前收到的更新令牌，必选项。<br>scope：表示申请的授权范围，不可以超出上一次申请的范围，如果省略该参数，则表示与上一次一致。</p>
<p>计算expires_in到期时间的方法：7776000<br>long longTime = System.currentTimeMillis() + Long.parseLong(“604800000”) * 1000;<br>        System.out.println(longTime);<br>        Date date = new Date(longTime);<br>        System.out.println(date);</p>
<p>密码模式<br>密码模式（Resource Owner Password Credentials Grant）中，用户向客户端提供自己的用户名和密码。客户端使用这些信息，向”服务商提供商”索要授权。<br>在这种模式中，用户必须把自己的密码给客户端，但是客户端不得储存密码。这通常用在用户对客户端高度信任的情况下，比如客户端是操作系统的一部分，或者由一个著名公司出品。而认证服务器只有在其他授权模式无法执行的情况下，才能考虑使用这种模式。<br>客户端模式<br>客户端模式（Client Credentials Grant）指客户端以自己的名义，而不是以用户的名义，向”服务提供商”进行认证。严格地说，客户端模式并不属于OAuth框架所要解决的问题。在这种模式中，用户直接向客户端注册，客户端以自己的名义要求”服务提供商”提供服务，其实不存在授权问题。</p>
<p>搭建OAuth<br>框架：<br>Spring Security for OAuth ?<br>Apis Authorization Server<br>Restlet Framework<br>Apache CXF</p>
<p>Shiro oauth??</p>
<p>一个OAuth2.0服务器端的实现 </p>
<p>数据库简单设计:<br>(可以不用数据库,access_token是临时的,放在内存中即可) </p>
<p>create table access_token (<br>id bigint,<br>access_token varchar(255) NOT NULL,<br>token_type varchar(255),<br>expires varchar(255),<br>refresh_token varchar(255),<br>username varchar(255),<br>client_id varchar(255),<br>createdtime datetime,<br>modifiedtime datetime<br>); </p>
<p>create table user (<br>uid bigint,<br>username varchar(255) NOT NULL,<br>password varchar(255) NOT NULL<br>); </p>
<p>create table client (<br>id bigint,<br>client_id varchar(255) NOT NULL,<br>client_secret varchar(255)<br>); </p>
<p>OAuth2.0协议主要是用access_token代替密码.<br>授权服务器保管用户密码并向第三方应用发放access_token,第三方应用接触不到用户<br>的密码.<br>资源服务器保管需要授权才能访问的资源(其实就是服务器提供的API),第三方应用凭<br>access_token访问资源服务器.(资源服务器还要问一下授权服务器这个access_token是<br>不是真的) </p>
<p>授权服务器做两件事: </p>
<ol>
<li>发放access_token的servlet </li>
<li>向资源服务器提供一个验证access_token真伪的WebService </li>
</ol>
<p>四种获取access token的方式<br>1    “Authorization Code” Grant<br>2    “Implicit” Grant<br>3    “Resource Owner Password Credentials” Grant<br>4    “Client Credentials” Grant  </p>
<ul>
<li>Refreshing an Access Token </li>
</ul>
<p>(在第一种方式里面要分两步获取access_token.) </p>
<p>class TokenServlet extends HttpServlet<br>{<br>     void doGet()<br>     {<br>         String response_type = request.getParameter(“response_type”);<br>         if (response_type.equals(“token”))<br>         {<br>              // TODO 第二种:”Implicit方式”;<br>         }<br>     } </p>
<pre><code>void doPost() 
{ 
    // TODO Filter Authorization: Basic base64(appkey:appsecret)  
// 验证 appkey appsecret

    String grant_type = request.getParameter(&quot;grant_type&quot;);     

    if (grant_type.equals(&quot;authorization_code&quot;)) 
    {             
        // TODO 第一种:&quot;Authorization Code方式&quot;;             
        return ; 
    } 

    if (grant_type.equals(&quot;password&quot;)) 
    {             
        // TODO 第三种:&quot;Password方式&quot;;             
        return ; 
    } 

    if (grant_type.equals(&quot;client_credentials&quot;)) 
    {             
        // TODO 第四种:&quot;Client Credentials方式&quot;;             
        return ; 
    } 

    if (grant_type.equals(&quot;refresh_token&quot;)) 
    {             
        // TODO 第五种:&quot;RefreshToken方式&quot;;             
        return ; 
    } 

} 
</code></pre><p>} </p>
<p>OAuth2.0存在的安全问题：</p>
<p>场景一：见demo—-&gt;访问信任页面A，授权后产生cookie，<br>在未退出或者未关闭浏览器的情况下携带A页面的cookie访问危险网站B<br>B站中要求访问页面A，A并不知道这是用户的请求还是跨站请求所以会同意访问，B达到了模拟用户操作的目的。</p>
<p>场景二：<br>通过Authorization code方式绑定QQ账号<br>（1）用户甲到第三方网站A登录后，到了绑定页面。此时还没绑定微博。<br>（2）绑定页面提供一个按钮：“绑定QQ”（地址a：<a href="http://aaa.com/index.xhtml?m=user_3rd_bind_qq）" target="_blank" rel="external">http://aaa.com/index.xhtml?m=user_3rd_bind_qq）</a><br>（3）用户甲点击地址a，程序生成如下地址b：<br><a href="https://api.weibo.com/oauth2/authorize?client_id=【9999999】&amp;redirect_uri=【http://aaa.comindex.xhtml?m=user_3rd_bind_qq_callback】&amp;response_type=【code】" target="_blank" rel="external">https://api.weibo.com/oauth2/authorize?client_id=【9999999】&amp;redirect_uri=【http://aaa.comindex.xhtml?m=user_3rd_bind_qq_callback】&amp;response_type=【code】</a><br>（4）用户甲浏览器定向到地址b，授权该应用。<br>（5）授权服务器根据传递的redirect_uri参数，组合认证参数code生成地址c：<br><a href="http://aaa.comindex.xhtml?m=user_3rd_bind_qq_callback&amp;code=【809ui0asduve】" target="_blank" rel="external">http://aaa.comindex.xhtml?m=user_3rd_bind_qq_callback&amp;code=【809ui0asduve】</a><br>（6）用户甲浏览器返回到地址c，完成绑定。<br>第5步中返回地址所带的code 与当前用户的关系。。。<br>两用户同时进行QQ绑定若没有各自特定的标识参数的后果。。。<br>可以在第五步做手脚交换二者的code 就可以形成交叉绑定</p>
<p>state: RECOMMENDED. An opaque value used by the client to maintain state between the request and callback.  The authorization server includes this value when redirecting the user-agent back to the client.  The parameter SHOULD be used for preventing cross-site request forgery as described in Section 10.12. </p>
<p>使用state参数用于请求阶段和回调阶段之间的状态保持可防止CSRF攻击和防止错误绑定<br>记录请求之前state，返回后比对—–&gt;安全<br>返回的state只能匹配固定几种url头部才通过验证—–&gt;方便</p>
<p>Spring Security for OAuth 是针对OAuth2基于Security 的实现，token存放于内存</p>
<p>Shiro的1.2版本提供接入oauth2，实现可以用Apache Oltu，有整合资料</p>
<p>Apache Oltu是OAuth协议的Java语言实现。<br>现在实现了oauth2.0标准的Java版本有：<br>1、Spring Security for OAuth<br>2、Apache Oltu(Apache Amber)<br>3、Apis Authorization Server<br>4、Restlet Framework<br>5、Apache CXF</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;公司用户中心系统不仅需要传统的注册登录，主流的快捷登录肯定也必不可少，第三方快捷登录的流程和相应系统数据表的设计就先不记录了，主要摘抄一下网上对oauth的介绍和自己的理解&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yzy755.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="第三方快捷登录" scheme="http://yzy755.github.io/tags/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BF%AB%E6%8D%B7%E7%99%BB%E5%BD%95/"/>
    
      <category term="oauth2.0" scheme="http://yzy755.github.io/tags/oauth2-0/"/>
    
  </entry>
  
  <entry>
    <title>基于shiro拓展并发登录人数控制</title>
    <link href="http://yzy755.github.io/2017/04/11/%E5%9F%BA%E4%BA%8Eshiro%E6%8B%93%E5%B1%95%E5%B9%B6%E5%8F%91%E7%99%BB%E5%BD%95%E4%BA%BA%E6%95%B0%E6%8E%A7%E5%88%B6/"/>
    <id>http://yzy755.github.io/2017/04/11/基于shiro拓展并发登录人数控制/</id>
    <published>2017-04-11T09:17:53.000Z</published>
    <updated>2017-04-13T03:44:08.041Z</updated>
    
    <content type="html"><![CDATA[<p>在公司做了个视频网站，当然视频是需要购买的，就像爱奇艺那样，所以同一用户就要有一个客户端登录个数的控制，不然一个用户买了，能让一个公司的人看- -！<a id="more"></a><br>spring security就直接提供了相应的功能；我的项目使用的权限框架是apach的shiro，Shiro的话没有提供默认实现，不过可以很容易的在Shiro中加入这个功能<br>我们来看下shiro拦截机制中filter的关系图<br><img src="/2017/04/11/基于shiro拓展并发登录人数控制/filter.png" alt="logo"><br>这里只介绍AccessControlFilter：<br>    AccessControlFilter提供了访问控制的基础功能；比如是否允许访问/当访问拒绝时如何处理等：<br>        abstract boolean isAccessAllowed(ServletRequest request, ServletResponse response, Object mappedValue) throws Exception;<br>        boolean onAccessDenied(ServletRequest request, ServletResponse response, Object mappedValue) throws Exception;<br>        abstract boolean onAccessDenied(ServletRequest request, ServletResponse response) throws Exception;</p>
<pre><code>    isAccessAllowed：表示是否允许访问；mappedValue就是[urls]配置中拦截器参数部分，如果允许访问返回true，否则false；
    onAccessDenied：表示当访问拒绝时是否已经处理了；如果返回true表示需要继续处理；如果返回false表示该拦截器实例已经处理了，将直接返回即可。

onPreHandle会自动调用这两个方法决定是否继续处理：
    boolean onPreHandle(ServletRequest request, ServletResponse response, Object mappedValue) throws Exception {  
        return isAccessAllowed(request, response, mappedValue) || onAccessDenied(request, response, mappedValue);  
    }   

另外AccessControlFilter还提供了如下方法用于处理如登录成功后/重定向到上一个请求： 
    void setLoginUrl(String loginUrl) //身份验证时使用，默认/login.jsp  
    String getLoginUrl()  
    Subject getSubject(ServletRequest request, ServletResponse response) //获取Subject实例  
    boolean isLoginRequest(ServletRequest request, ServletResponse response)//当前请求是否是登录请求  
    void saveRequestAndRedirectToLogin(ServletRequest request, ServletResponse response) throws IOException //将当前请求保存起来并重定向到登录页面  
    void saveRequest(ServletRequest request) //将请求保存起来，如登录成功后再重定向回该请求  
    void redirectToLogin(ServletRequest request, ServletResponse response) //重定向到登录页面 
    比如基于表单的身份验证就需要使用这些功能。
</code></pre><p>如果我们想进行访问访问的控制就可以继承AccessControlFilter；如果我们要添加一些通用数据我们可以直接继承PathMatchingFilter。所以定义一个类继承AccessControlFilter重写onAccessDenied方法，isAccessAllowed方法直接返回false，使程序进入拒绝访问的处理过程，也就是onAccessDenied方法，在这个方法中，我们的思路就是<br>    1.使用链表记录同一用户的登录sessionid，用shiro提供的cache记录用户和session的映射关系，用户名为key,链表为value<br>    2.用户登录后使用subject拿到用户名，以用户名取出当前服务器存储的sessionid链表，判断链表中有无当前登录的sessionid，无则push<br>    3.判断链表的的长度，若大于系统允许的登录数，链表进行移除操作，获得移除的session并标记为被踢出（移除的顺序可配置为踢前者或者后者）<br>    4.被移除的session（客户端）再次访问系统页面时，走完上述过程发现自己有被踢出的标记，调用subject.logout()方法，并重定向到踢出的提示页面<br>帖一波代码：</p>
<pre><code>public class KickoutSessionControlFilter extends AccessControlFilter
{
    private Logger logger = LoggerFactory.getLogger(KickoutSessionControlFilter.class);
    private String kickoutUrl; // 踢出后到的地址
    private boolean kickoutAfter; // 踢出之前登录的或之后登录的用户 (在配置文件中配)
    private int maxSession = 1; // 同一个帐号最大会话数 默认1

    private SessionManager sessionManager;

    private Cache&lt;String, Deque&lt;Serializable&gt;&gt; cache;
    @Override
    protected boolean isAccessAllowed(ServletRequest request, ServletResponse response, Object mappedValue) throws Exception
    {
        return false;
    }

    @Override
    protected boolean onAccessDenied(ServletRequest request, ServletResponse response) throws Exception
    {
        Subject subject = getSubject(request, response);
        if (!subject.isAuthenticated() &amp;&amp; !subject.isRemembered())
        {
            // 如果没有登录，直接进行之后的流程
            return true;
        }

        Session session = subject.getSession();
        String username = (String) subject.getPrincipal();
        Serializable sessionId = session.getId();

        // 同步控制
        Deque&lt;Serializable&gt; deque = cache.get(username);
        if (deque == null)
        {
            deque = new LinkedList&lt;Serializable&gt;();
        }

        // 如果队列里没有此sessionId，且用户没有被踢出；放入队列
        if (!deque.contains(sessionId) &amp;&amp; session.getAttribute(&quot;kickout&quot;) == null)
        {
            deque.push(sessionId);
        }

        // 如果队列里的sessionId数超出最大会话数，开始踢人
        while (deque.size() &gt; maxSession)
        {
            Serializable kickoutSessionId = null;
            if (kickoutAfter)
            { // 如果踢出后者
                kickoutSessionId = deque.removeFirst();
            }
            else
            { // 否则踢出前者
                kickoutSessionId = deque.removeLast();
            }
            try
            {
                Session kickoutSession = sessionManager.getSession(new DefaultSessionKey(kickoutSessionId));
                if (kickoutSession != null)
                {
                    // 设置会话的kickout属性表示踢出了
                    kickoutSession.setAttribute(&quot;kickout&quot;, true);
                }
            }
            catch (Exception e)
            {
                e.printStackTrace();
                logger.error(e.getMessage(), e);
            }
        }
        cache.put(username, deque);

        // 如果被踢出了，直接退出，重定向到踢出后的地址
        if (session.getAttribute(&quot;kickout&quot;) != null)
        {
            try
            {
                subject.logout();
            }
            catch (Exception e)
            {
                e.printStackTrace();
                logger.error(e.getMessage(), e);
            }
            saveRequest(request);
            WebUtils.issueRedirect(request, response, kickoutUrl);
            return false;
        }
        return true;
    }

    public void setKickoutUrl(String kickoutUrl)
    {
        this.kickoutUrl = kickoutUrl;
    }

    public void setKickoutAfter(boolean kickoutAfter)
    {
        this.kickoutAfter = kickoutAfter;
    }

    public void setMaxSession(int maxSession)
    {
        this.maxSession = maxSession;
    }

    public void setSessionManager(SessionManager sessionManager)
    {
        this.sessionManager = sessionManager;
    }

    public void setCacheManager(CacheManager cacheManager)
    {
        this.cache = cacheManager.getCache(&quot;shiro-kickout-session&quot;);
    }
}
</code></pre><p>当然上述的实现离不开shiro的正确配置，帖一波配置：</p>
<pre><code>&lt;bean id=&quot;securityManager&quot; class=&quot;org.apache.shiro.web.mgt.DefaultWebSecurityManager&quot;&gt;
    &lt;property name=&quot;realms&quot;&gt;
        &lt;util:list id=&quot;beanList&quot;&gt;
            &lt;ref bean=&quot;myCasRealm&quot; /&gt;
            &lt;ref bean=&quot;myLoginRealm&quot; /&gt;
        &lt;/util:list&gt;
    &lt;/property&gt;
    &lt;!-- &lt;property name=&quot;realm&quot; ref=&quot;myCasRealm&quot; /&gt; --&gt;
    &lt;property name=&quot;subjectFactory&quot; ref=&quot;casSubjectFactory&quot; /&gt;
    &lt;property name=&quot;cacheManager&quot; ref=&quot;cacheManager&quot; /&gt;
    &lt;property name=&quot;sessionManager&quot; ref=&quot;sessionManager&quot; /&gt;
&lt;/bean&gt;

&lt;!-- 如果要实现cas的remember me的功能，需要用到下面这个bean，并设置到securityManager的subjectFactory中 --&gt;
&lt;bean id=&quot;casSubjectFactory&quot; class=&quot;org.apache.shiro.cas.CasSubjectFactory&quot; /&gt;

&lt;!-- 項目自定义的Realm --&gt;
&lt;bean id=&quot;myCasRealm&quot; class=&quot;com.booway.shiro.MyCasRealm&quot;&gt;
    &lt;property name=&quot;cacheManager&quot; ref=&quot;cacheManager&quot; /&gt;
    &lt;property name=&quot;casServerUrlPrefix&quot; value=&quot;${cas.server.url}&quot; /&gt;
    &lt;!-- 客户端的回调地址设置，必须和下面的shiro-cas过滤器拦截的地址一致 --&gt;
    &lt;property name=&quot;casService&quot; value=&quot;/cas&quot; /&gt;
&lt;/bean&gt;

&lt;!-- 項目自定义的Realm --&gt;
&lt;bean id=&quot;myLoginRealm&quot; class=&quot;com.booway.shiro.MyLoginRealm&quot;&gt;
    &lt;property name=&quot;cacheManager&quot; ref=&quot;cacheManager&quot; /&gt;
    &lt;property name=&quot;casServerUrlPrefix&quot; value=&quot;${cas.server.url}&quot; /&gt;
    &lt;!-- 客户端的回调地址设置，必须和下面的shiro-cas过滤器拦截的地址一致 --&gt;
    &lt;property name=&quot;casService&quot; value=&quot;/cas&quot; /&gt;
&lt;/bean&gt;

&lt;!-- 用户授权信息Cache --&gt;
&lt;bean id=&quot;cacheManager&quot; class=&quot;org.apache.shiro.cache.MemoryConstrainedCacheManager&quot; /&gt;

&lt;!-- 会话ID生成器 --&gt;
&lt;bean id=&quot;sessionIdGenerator&quot; class=&quot;org.apache.shiro.session.mgt.eis.JavaUuidSessionIdGenerator&quot;/&gt;

&lt;!-- 会话DAO --&gt;
&lt;bean id=&quot;sessionDAO&quot; class=&quot;org.apache.shiro.session.mgt.eis.EnterpriseCacheSessionDAO&quot;&gt;
    &lt;property name=&quot;activeSessionsCacheName&quot; value=&quot;shiro-activeSessionCache&quot;/&gt;
    &lt;property name=&quot;sessionIdGenerator&quot; ref=&quot;sessionIdGenerator&quot;/&gt;
&lt;/bean&gt;

&lt;!-- 会话Cookie模板 --&gt;
&lt;bean id=&quot;sessionIdCookie&quot; class=&quot;org.apache.shiro.web.servlet.SimpleCookie&quot;&gt;
    &lt;constructor-arg value=&quot;sid&quot;/&gt;
    &lt;property name=&quot;httpOnly&quot; value=&quot;true&quot;/&gt;
    &lt;property name=&quot;maxAge&quot; value=&quot;3600&quot;/&gt;
&lt;/bean&gt;

&lt;!-- 会话管理器 --&gt;
&lt;bean id=&quot;sessionManager&quot; class=&quot;org.apache.shiro.web.session.mgt.DefaultWebSessionManager&quot;&gt;
    &lt;property name=&quot;cacheManager&quot; ref=&quot;cacheManager&quot; /&gt;
    &lt;property name=&quot;globalSessionTimeout&quot; value=&quot;1800000&quot;/&gt;
    &lt;property name=&quot;deleteInvalidSessions&quot; value=&quot;true&quot;/&gt;
    &lt;property name=&quot;sessionValidationSchedulerEnabled&quot; value=&quot;true&quot;/&gt;
    &lt;property name=&quot;sessionValidationScheduler&quot; ref=&quot;sessionValidationScheduler&quot;/&gt;
    &lt;property name=&quot;sessionDAO&quot; ref=&quot;sessionDAO&quot;/&gt;
    &lt;property name=&quot;sessionIdCookieEnabled&quot; value=&quot;true&quot;/&gt;
    &lt;property name=&quot;sessionIdCookie&quot; ref=&quot;sessionIdCookie&quot;/&gt;
&lt;/bean&gt;

&lt;!-- 会话验证调度器 --&gt;
&lt;bean id=&quot;sessionValidationScheduler&quot; class=&quot;org.apache.shiro.session.mgt.quartz.QuartzSessionValidationScheduler&quot;&gt;
    &lt;property name=&quot;sessionValidationInterval&quot; value=&quot;1800000&quot;/&gt;
    &lt;property name=&quot;sessionManager&quot; ref=&quot;sessionManager&quot;/&gt;
&lt;/bean&gt;

&lt;bean id=&quot;kickoutSessionControlFilter&quot;
    class=&quot;com.booway.shiro.KickoutSessionControlFilter&quot;&gt;
    &lt;property name=&quot;cacheManager&quot; ref=&quot;cacheManager&quot; /&gt;
    &lt;property name=&quot;sessionManager&quot; ref=&quot;sessionManager&quot; /&gt;

    &lt;property name=&quot;kickoutAfter&quot; value=&quot;false&quot; /&gt;
    &lt;property name=&quot;maxSession&quot; value=&quot;1&quot; /&gt;
    &lt;property name=&quot;kickoutUrl&quot; value=&quot;/kickout.xhtml&quot; /&gt;
&lt;/bean&gt;

&lt;!-- Shiro Filter --&gt;
&lt;bean id=&quot;shiroFilter&quot; class=&quot;com.booway.shiro.ShiroFilterFactoryBean&quot;&gt;
    &lt;property name=&quot;securityManager&quot; ref=&quot;securityManager&quot; /&gt;
    &lt;!-- 用于调用Controller --&gt;
    &lt;property name=&quot;service&quot; value=&quot;${uam.login.failurl}&quot; /&gt;
    &lt;property name=&quot;loginUrl&quot; value=&quot;${cas.server.login.url}?service=&quot; /&gt;
    &lt;!-- 用户访问未授权页面后跳转的错误信息页面 --&gt;
    &lt;property name=&quot;unauthorizedUrl&quot; value=&quot;/401.html&quot;&gt;&lt;/property&gt;

    &lt;!-- 自己实现的formAuthcFilter，加入key type --&gt;
    &lt;property name=&quot;filters&quot;&gt;
        &lt;util:map&gt;
            &lt;entry key=&quot;casFilter&quot; value-ref=&quot;casFilter&quot;&gt;
            &lt;/entry&gt;
            &lt;entry key=&quot;singleSignOutFilter&quot; value-ref=&quot;singleSignOutFilter&quot;&gt;
            &lt;/entry&gt;
            &lt;entry key=&quot;roles&quot; value-ref=&quot;roleFilter&quot;&gt;
            &lt;/entry&gt;
            &lt;entry key=&quot;kickout&quot; value-ref=&quot;kickoutSessionControlFilter&quot;&gt;
            &lt;/entry&gt;
        &lt;/util:map&gt;
    &lt;/property&gt;

    &lt;property name=&quot;filterChainDefinitions&quot;&gt;
        &lt;value&gt;
            /** = singleSignOutFilter,casFilter,kickout

        &lt;/value&gt;
    &lt;/property&gt;
&lt;/bean&gt;

&lt;bean id=&quot;casFilter&quot; class=&quot;com.booway.shiro.MyCasFilter&quot;&gt;
    &lt;!-- 配置验证错误时的失败页面 --&gt;
    &lt;property name=&quot;failureUrl&quot; value=&quot;${uam.login.failurl}&quot; /&gt;
    &lt;property name=&quot;successUrl&quot; value=&quot;/index.xhtml&quot; /&gt;
    &lt;property name=&quot;casLoginUrl&quot; value=&quot;${cas.server.login.url}&quot; /&gt;
    &lt;property name=&quot;interceptUrlPrefix&quot;&gt;
        &lt;list&gt;

        &lt;/list&gt;
    &lt;/property&gt;

    &lt;property name=&quot;loginUrls&quot;&gt;
        &lt;list&gt;
            &lt;value&gt;/index/login.xhtml&lt;/value&gt;
        &lt;/list&gt;
    &lt;/property&gt;
    &lt;!-- &lt;property name=&quot;excludeUrls&quot;&gt; &lt;list&gt; &lt;value&gt;/index.xhtml&lt;/value&gt; &lt;/list&gt; 
        &lt;/property&gt; --&gt;
&lt;/bean&gt;

&lt;bean id=&quot;userFilter&quot; class=&quot;com.booway.shiro.MyUserFilter&quot;&gt;
    &lt;!-- 配置验证错误时的失败页面 --&gt;
    &lt;property name=&quot;failureUrl&quot; value=&quot;${uam.login.failurl}&quot; /&gt;

&lt;/bean&gt;

&lt;bean id=&quot;logoutFilter&quot; class=&quot;com.booway.shiro.MyLogoutFilter&quot;&gt;
&lt;/bean&gt;

&lt;bean id=&quot;roleFilter&quot; class=&quot;com.booway.shiro.RolesAuthorizationFilter&quot;&gt;
&lt;/bean&gt;

&lt;!-- 退出过滤器 --&gt;
&lt;bean id=&quot;singleSignOutFilter&quot; class=&quot;com.booway.shiro.CustomSingleSignOutFilter&quot;&gt;
&lt;/bean&gt;

&lt;!-- 保证实现了Shiro内部lifecycle函数的bean执行 --&gt;
&lt;bean id=&quot;lifecycleBeanPostProcessor&quot; class=&quot;org.apache.shiro.spring.LifecycleBeanPostProcessor&quot; /&gt;

&lt;!-- AOP式方法级权限检查 --&gt;
&lt;bean
    class=&quot;org.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator&quot;
    depends-on=&quot;lifecycleBeanPostProcessor&quot;&gt;
    &lt;property name=&quot;proxyTargetClass&quot; value=&quot;true&quot; /&gt;
&lt;/bean&gt;

&lt;bean
    class=&quot;org.springframework.beans.factory.config.MethodInvokingFactoryBean&quot;&gt;
    &lt;property name=&quot;staticMethod&quot;
        value=&quot;org.apache.shiro.SecurityUtils.setSecurityManager&quot; /&gt;
    &lt;property name=&quot;arguments&quot; ref=&quot;securityManager&quot; /&gt;
&lt;/bean&gt;

&lt;!-- 开启Shiro的注解(如@RequiresRoles,@RequiresPermissions), 需借助SpringAOP扫描使用Shiro注解的类,并在必要时进行安全逻辑验证 --&gt;
&lt;bean
    class=&quot;org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor&quot;&gt;
    &lt;property name=&quot;securityManager&quot; ref=&quot;securityManager&quot; /&gt;
&lt;/bean&gt;
</code></pre><p>这样一个shiro的拓展就完成啦！</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在公司做了个视频网站，当然视频是需要购买的，就像爱奇艺那样，所以同一用户就要有一个客户端登录个数的控制，不然一个用户买了，能让一个公司的人看- -！&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yzy755.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="shiro" scheme="http://yzy755.github.io/tags/shiro/"/>
    
  </entry>
  
  <entry>
    <title>jvm调优总结 -Xms -Xmx -Xmn -Xss</title>
    <link href="http://yzy755.github.io/2017/03/24/jvm%E8%B0%83%E4%BC%98%E6%80%BB%E7%BB%93%20-Xms%20-Xmx%20-Xmn%20-Xss/"/>
    <id>http://yzy755.github.io/2017/03/24/jvm调优总结 -Xms -Xmx -Xmn -Xss/</id>
    <published>2017-03-24T09:50:48.000Z</published>
    <updated>2017-03-24T10:09:32.023Z</updated>
    
    <content type="html"><![CDATA[<p>堆大小设置<br>JVM 中最大堆大小有三方面限制：相关操作系统的数据模型（32-bt还是64-bit）限制；系统的可用虚拟内存限制；系统的可用物理内存限制。32位系统下，一般限制在1.5G~2G；64为操作系统对内存无限制。我在Windows Server 2003 系统，3.5G物理内存，JDK5.0下测试，最大可设置为1478m。<br><a id="more"></a><br>典型设置：<br>java -Xmx3550m -Xms3550m -Xmn2g -Xss128k<br>-Xmx3550m：设置JVM最大可用内存为3550M。<br>-Xms3550m：设置JVM促使内存为3550m。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。<br>-Xmn2g：设置年轻代大小为2G。整个JVM内存大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。<br>-Xss128k：设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。更具应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。<br>java -Xmx3550m -Xms3550m -Xss128k -XX:NewRatio=4 -XX:SurvivorRatio=4 -XX:MaxPermSize=16m -XX:MaxTenuringThreshold=0<br>-XX:NewRatio=4:设置年轻代（包括Eden和两个Survivor区）与年老代的比值（除去持久代）。设置为4，则年轻代与年老代所占比值为1：4，年轻代占整个堆栈的1/5<br>-XX:SurvivorRatio=4：设置年轻代中Eden区与Survivor区的大小比值。设置为4，则两个Survivor区与一个Eden区的比值为2:4，一个Survivor区占整个年轻代的1/6<br>-XX:MaxPermSize=16m:设置持久代大小为16m。<br>-XX:MaxTenuringThreshold=0：设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概论。</p>
<p>回收器选择<br>JVM给了三种选择：串行收集器、并行收集器、并发收集器，但是串行收集器只适用于小数据量的情况，所以这里的选择主要针对并行收集器和并发收集器。默认情况下，JDK5.0以前都是使用串行收集器，如果想使用其他收集器需要在启动时加入相应参数。JDK5.0以后，JVM会根据当前系统配置进行判断。<br>吞吐量优先的并行收集器<br>如上文所述，并行收集器主要以到达一定的吞吐量为目标，适用于科学技术和后台处理等。<br>典型配置：<br>java -Xmx3800m -Xms3800m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:ParallelGCThreads=20<br>-XX:+UseParallelGC：选择垃圾收集器为并行收集器。此配置仅对年轻代有效。即上述配置下，年轻代使用并发收集，而年老代仍旧使用串行收集。<br>-XX:ParallelGCThreads=20：配置并行收集器的线程数，即：同时多少个线程一起进行垃圾回收。此值最好配置与处理器数目相等。<br>java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:ParallelGCThreads=20 -XX:+UseParallelOldGC<br>-XX:+UseParallelOldGC：配置年老代垃圾收集方式为并行收集。JDK6.0支持对年老代并行收集。<br>java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC  -XX:MaxGCPauseMillis=100<br>-XX:MaxGCPauseMillis=100:设置每次年轻代垃圾回收的最长时间，如果无法满足此时间，JVM会自动调整年轻代大小，以满足此值。<br>java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC  -XX:MaxGCPauseMillis=100 -XX:+UseAdaptiveSizePolicy<br>-XX:+UseAdaptiveSizePolicy：设置此选项后，并行收集器会自动选择年轻代区大小和相应的Survivor区比例，以达到目标系统规定的最低相应时间或者收集频率等，此值建议使用并行收集器时，一直打开。<br>响应时间优先的并发收集器<br>如上文所述，并发收集器主要是保证系统的响应时间，减少垃圾收集时的停顿时间。适用于应用服务器、电信领域等。<br>典型配置：<br>java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:ParallelGCThreads=20 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC<br>-XX:+UseConcMarkSweepGC：设置年老代为并发收集。测试中配置这个以后，-XX:NewRatio=4的配置失效了，原因不明。所以，此时年轻代大小最好用-Xmn设置。<br>-XX:+UseParNewGC:设置年轻代为并行收集。可与CMS收集同时使用。JDK5.0以上，JVM会根据系统配置自行设置，所以无需再设置此值。<br>java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseConcMarkSweepGC -XX:CMSFullGCsBeforeCompaction=5 -XX:+UseCMSCompactAtFullCollection<br>-XX:CMSFullGCsBeforeCompaction：由于并发收集器不对内存空间进行压缩、整理，所以运行一段时间以后会产生“碎片”，使得运行效率降低。此值设置运行多少次GC以后对内存空间进行压缩、整理。<br>-XX:+UseCMSCompactAtFullCollection：打开对年老代的压缩。可能会影响性能，但是可以消除碎片</p>
<p>辅助信息<br>JVM提供了大量命令行参数，打印信息，供调试使用。主要有以下一些：<br>-XX:+PrintGC<br>输出形式：[GC 118250K-&gt;113543K(130112K), 0.0094143 secs]<br>                [Full GC 121376K-&gt;10414K(130112K), 0.0650971 secs]<br>-XX:+PrintGCDetails<br>输出形式：[GC [DefNew: 8614K-&gt;781K(9088K), 0.0123035 secs] 118250K-&gt;113543K(130112K), 0.0124633 secs]<br>                [GC [DefNew: 8614K-&gt;8614K(9088K), 0.0000665 secs][Tenured: 112761K-&gt;10414K(121024K), 0.0433488 secs] 121376K-&gt;10414K(130112K), 0.0436268 secs]<br>-XX:+PrintGCTimeStamps -XX:+PrintGC：PrintGCTimeStamps可与上面两个混合使用<br>输出形式：11.851: [GC 98328K-&gt;93620K(130112K), 0.0082960 secs]<br>-XX:+PrintGCApplicationConcurrentTime:打印每次垃圾回收前，程序未中断的执行时间。可与上面混合使用<br>输出形式：Application time: 0.5291524 seconds<br>-XX:+PrintGCApplicationStoppedTime：打印垃圾回收期间程序暂停的时间。可与上面混合使用<br>输出形式：Total time for which application threads were stopped: 0.0468229 seconds<br>-XX:PrintHeapAtGC:打印GC前后的详细堆栈信息<br>输出形式：<br>34.702: [GC {Heap before gc invocations=7:<br> def new generation   total 55296K, used 52568K [0x1ebd0000, 0x227d0000, 0x227d0000)<br>eden space 49152K,  99% used [0x1ebd0000, 0x21bce430, 0x21bd0000)<br>from space 6144K,  55% used [0x221d0000, 0x22527e10, 0x227d0000)<br>  to   space 6144K,   0% used [0x21bd0000, 0x21bd0000, 0x221d0000)<br> tenured generation   total 69632K, used 2696K [0x227d0000, 0x26bd0000, 0x26bd0000)<br>the space 69632K,   3% used [0x227d0000, 0x22a720f8, 0x22a72200, 0x26bd0000)<br> compacting perm gen  total 8192K, used 2898K [0x26bd0000, 0x273d0000, 0x2abd0000)<br>   the space 8192K,  35% used [0x26bd0000, 0x26ea4ba8, 0x26ea4c00, 0x273d0000)<br>    ro space 8192K,  66% used [0x2abd0000, 0x2b12bcc0, 0x2b12be00, 0x2b3d0000)<br>    rw space 12288K,  46% used [0x2b3d0000, 0x2b972060, 0x2b972200, 0x2bfd0000)<br>34.735: [DefNew: 52568K-&gt;3433K(55296K), 0.0072126 secs] 55264K-&gt;6615K(124928K)Heap after gc invocations=8:<br> def new generation   total 55296K, used 3433K [0x1ebd0000, 0x227d0000, 0x227d0000)<br>eden space 49152K,   0% used [0x1ebd0000, 0x1ebd0000, 0x21bd0000)<br>  from space 6144K,  55% used [0x21bd0000, 0x21f2a5e8, 0x221d0000)<br>  to   space 6144K,   0% used [0x221d0000, 0x221d0000, 0x227d0000)<br> tenured generation   total 69632K, used 3182K [0x227d0000, 0x26bd0000, 0x26bd0000)<br>the space 69632K,   4% used [0x227d0000, 0x22aeb958, 0x22aeba00, 0x26bd0000)<br> compacting perm gen  total 8192K, used 2898K [0x26bd0000, 0x273d0000, 0x2abd0000)<br>   the space 8192K,  35% used [0x26bd0000, 0x26ea4ba8, 0x26ea4c00, 0x273d0000)<br>    ro space 8192K,  66% used [0x2abd0000, 0x2b12bcc0, 0x2b12be00, 0x2b3d0000)<br>    rw space 12288K,  46% used [0x2b3d0000, 0x2b972060, 0x2b972200, 0x2bfd0000)<br>}<br>, 0.0757599 secs]<br>-Xloggc:filename:与上面几个配合使用，把相关日志信息记录到文件以便分析。</p>
<p>常见配置汇总<br>堆设置<br>-Xms:初始堆大小<br>-Xmx:最大堆大小<br>-XX:NewSize=n:设置年轻代大小<br>-XX:NewRatio=n:设置年轻代和年老代的比值。如:为3，表示年轻代与年老代比值为1：3，年轻代占整个年轻代年老代和的1/4<br>-XX:SurvivorRatio=n:年轻代中Eden区与两个Survivor区的比值。注意Survivor区有两个。如：3，表示Eden：Survivor=3：2，一个Survivor区占整个年轻代的1/5<br>-XX:MaxPermSize=n:设置持久代大小<br>收集器设置<br>-XX:+UseSerialGC:设置串行收集器<br>-XX:+UseParallelGC:设置并行收集器<br>-XX:+UseParalledlOldGC:设置并行年老代收集器<br>-XX:+UseConcMarkSweepGC:设置并发收集器<br>垃圾回收统计信息<br>-XX:+PrintGC<br>-XX:+PrintGCDetails<br>-XX:+PrintGCTimeStamps<br>-Xloggc:filename<br>并行收集器设置<br>-XX:ParallelGCThreads=n:设置并行收集器收集时使用的CPU数。并行收集线程数。<br>-XX:MaxGCPauseMillis=n:设置并行收集最大暂停时间<br>-XX:GCTimeRatio=n:设置垃圾回收时间占程序运行时间的百分比。公式为1/(1+n)<br>并发收集器设置<br>-XX:+CMSIncrementalMode:设置为增量模式。适用于单CPU情况。<br>-XX:ParallelGCThreads=n:设置并发收集器年轻代收集方式为并行收集时，使用的CPU数。并行收集线程数。</p>
<p>四、调优总结<br>年轻代大小选择<br>响应时间优先的应用：尽可能设大，直到接近系统的最低响应时间限制（根据实际情况选择）。在此种情况下，年轻代收集发生的频率也是最小的。同时，减少到达年老代的对象。<br>吞吐量优先的应用：尽可能的设置大，可能到达Gbit的程度。因为对响应时间没有要求，垃圾收集可以并行进行，一般适合8CPU以上的应用。<br>年老代大小选择<br>响应时间优先的应用：年老代使用并发收集器，所以其大小需要小心设置，一般要考虑并发会话率和会话持续时间等一些参数。如果堆设置小了，可以会造成内存碎片、高回收频率以及应用暂停而使用传统的标记清除方式；如果堆大了，则需要较长的收集时间。最优化的方案，一般需要参考以下数据获得：<br>并发垃圾收集信息<br>持久代并发收集次数<br>传统GC信息<br>花在年轻代和年老代回收上的时间比例<br>减少年轻代和年老代花费的时间，一般会提高应用的效率<br>吞吐量优先的应用：一般吞吐量优先的应用都有一个很大的年轻代和一个较小的年老代。原因是，这样可以尽可能回收掉大部分短期对象，减少中期的对象，而年老代尽存放长期存活对象。<br>较小堆引起的碎片问题<br>因为年老代的并发收集器使用标记、清除算法，所以不会对堆进行压缩。当收集器回收时，他会把相邻的空间进行合并，这样可以分配给较大的对象。但是，当堆空间较小时，运行一段时间以后，就会出现“碎片”，如果并发收集器找不到足够的空间，那么并发收集器将会停止，然后使用传统的标记、清除方式进行回收。如果出现“碎片”，可能需要进行如下配置：<br>-XX:+UseCMSCompactAtFullCollection：使用并发收集器时，开启对年老代的压缩。<br>-XX:CMSFullGCsBeforeCompaction=0：上面配置开启的情况下，这里设置多少次Full GC后，对年老代进行压缩</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;堆大小设置&lt;br&gt;JVM 中最大堆大小有三方面限制：相关操作系统的数据模型（32-bt还是64-bit）限制；系统的可用虚拟内存限制；系统的可用物理内存限制。32位系统下，一般限制在1.5G~2G；64为操作系统对内存无限制。我在Windows Server 2003 系统，3.5G物理内存，JDK5.0下测试，最大可设置为1478m。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yzy755.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="jvm" scheme="http://yzy755.github.io/tags/jvm/"/>
    
      <category term="调优" scheme="http://yzy755.github.io/tags/%E8%B0%83%E4%BC%98/"/>
    
  </entry>
  
  <entry>
    <title>jvm内存分配</title>
    <link href="http://yzy755.github.io/2017/03/24/jvm%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/"/>
    <id>http://yzy755.github.io/2017/03/24/jvm内存分配/</id>
    <published>2017-03-24T07:56:48.000Z</published>
    <updated>2017-03-24T10:09:41.798Z</updated>
    
    <content type="html"><![CDATA[<pre><code>java内存组成介绍：堆(Heap)和非堆(Non-heap)内存

   按照官方的说法：“Java 虚拟机具有一个堆，堆是运行时数据区域，所有类实例和数组的内存均从此处分配。堆是在 Java 虚拟机启动时创建的。”“在JVM中堆之外的内存称为非堆内存(Non-heap memory)”。可以看出JVM主要管理两种类型的内存：堆和非堆。简单来说堆就是Java代码可及的内存，是留给开发人员使用的；非堆就是JVM留给 自己用的，所以方法区、JVM内部处理或优化所需的内存(如JIT编译后的代码缓存)、每个类结构(如运行时常数池、字段和方法数据)以及方法和构造方法 的代码都在非堆内存中。
</code></pre><a id="more"></a>
<p><img src="/2017/03/24/jvm内存分配/combination.PNG" alt="logo"><br>               方法栈&amp;本地方法栈:<br>            线程创建时产生,方法执行时生成栈帧</p>
<pre><code>        方法区
        存储类的元数据信息 常量等

        堆
        java代码中所有的new操作
        native Memory(C heap)
        Direct Bytebuffer JNI Compile GC;


堆内存分配

   JVM初始分配的内存由-Xms指定，默认是物理内存的1/64；JVM最大分配的内存由-Xmx指 定，默认是物理内存的1/4。默认空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制；空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制。因此服务器一般设置-Xms、-Xmx相等以避免在每次GC 后调整堆的大小。对象的堆内存由称为垃圾回收器的自动内存管理系统回收。
</code></pre><p><img src="/2017/03/24/jvm内存分配/allocation.PNG" alt="logo"></p>
<pre><code>      组成                    详解
      Young Generation        即图中的Eden + From Space + To Space
      Eden                    存放新生的对象
      Survivor Space            有两个，存放每次垃圾回收后存活的对象
      Old Generation            Tenured Generation 即图中的Old Space 主要存放应用程序中生命周期长的存活对象

非堆内存分配
JVM使用-XX:PermSize设置非堆内存初始值，默认是物理内存的1/64；由XX:MaxPermSize设置最大非堆内存的大小，默认是物理内存的1/4。

      组成                    详解
      Permanent Generation    保存虚拟机自己的静态(refective)数据
                              主要存放加载的Class类级别静态对象如class本身，method，field等等
                              permanent generation空间不足会引发full GC(详见HotSpot VM GC种类)
      Code Cache                用于编译和保存本地代码（native code）的内存
                              JVM内部处理或优化

JVM内存限制(最大值)

JVM内存的最大值跟操作系统有很大的关系。简单的说就32位处理器虽然 可控内存空间有4GB,但是具体的操作系统会给一个限制，这个限制一般是2GB-3GB（一般来说Windows系统下为1.5G-2G，Linux系统 下为2G-3G），而64bit以上的处理器就不会有限制了。
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;java内存组成介绍：堆(Heap)和非堆(Non-heap)内存

   按照官方的说法：“Java 虚拟机具有一个堆，堆是运行时数据区域，所有类实例和数组的内存均从此处分配。堆是在 Java 虚拟机启动时创建的。”“在JVM中堆之外的内存称为非堆内存(Non-heap memory)”。可以看出JVM主要管理两种类型的内存：堆和非堆。简单来说堆就是Java代码可及的内存，是留给开发人员使用的；非堆就是JVM留给 自己用的，所以方法区、JVM内部处理或优化所需的内存(如JIT编译后的代码缓存)、每个类结构(如运行时常数池、字段和方法数据)以及方法和构造方法 的代码都在非堆内存中。
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yzy755.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="jvm" scheme="http://yzy755.github.io/tags/jvm/"/>
    
      <category term="内存分配" scheme="http://yzy755.github.io/tags/%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/"/>
    
  </entry>
  
  <entry>
    <title>linux环境下ffmpeg加入libx264编译方法</title>
    <link href="http://yzy755.github.io/2017/03/10/linux%E7%8E%AF%E5%A2%83%E4%B8%8Bffmpeg%E5%8A%A0%E5%85%A5libx264%E7%BC%96%E8%AF%91%E6%96%B9%E6%B3%95/"/>
    <id>http://yzy755.github.io/2017/03/10/linux环境下ffmpeg加入libx264编译方法/</id>
    <published>2017-03-10T08:50:48.000Z</published>
    <updated>2017-03-24T10:23:22.503Z</updated>
    
    <content type="html"><![CDATA[<p>下载源码<br>  libx264 <a href="http://download.videolan.org/x264/snapshots/" target="_blank" rel="external">http://download.videolan.org/x264/snapshots/</a> 最新版<br>  yasm <a href="http://yasm.tortall.net/Download.html" target="_blank" rel="external">http://yasm.tortall.net/Download.html</a> 版本为yasm-1.3.0(yasm是汇编编译器,因为ffmpeg中为了提高效率用到了汇编指令)<br>  ffmpeg <a href="http://www.ffmpeg.org/download.html" target="_blank" rel="external">http://www.ffmpeg.org/download.html</a> 版本为ffmpge-2.6.3<br><a id="more"></a><br>编译</p>
<ol>
<li><p>yasm<br>解压yasm-1.3.0至当前目录<br>tar -zxvf yasm.1.3.0.tar.gz<br>cd yasm<br>./configure –prefix=/usr/local/yasm<br>make<br>make install</p>
</li>
<li><p>libx264<br>解压x264-snapshot-20140424-2245.tar至当前目录<br>tar -zxvf x264-snapshot-20140424-2245.tar<br>cd x264<br>./configure –prefix=/usr/local/x264 –enable-shared –enable-static –enable-yasm<br>make<br>make install</p>
</li>
<li><p>ffmpeg<br>解压ffmpeg-2.8.0.tar至当前目录<br>tar -zxvf ffmpeg-2.8.0.tar<br>cd ffmpeg<br>./configure –prefix=/usr/local/ffmpeg –enable-shared –enable-libx264 –enable-gpl –enable-pthreads –extra-cflags=-I/usr/local/x264/include –extra-ldflags=-L/usr/local/x264/lib<br>make<br>make install<br>容易出现的问题<br>error while loading shared libraries: libpostproc.so.53（libx264.so.148）: cannot open shared object file: No such file or directory<br>运行 ldd $(which ffmpeg) 命令  ps:ldd命令用于判断某个可执行的 binary 档案含有什么动态函式库<br>在linux下安装ffmpeg出现此问题运行改命令示例：<br>linux-vdso.so.1 =&gt;  (0x00007fff6173b000)<br>   libavdevice.so.56 =&gt; /usr/local/ffmpeg/lib/libavdevice.so.56 (0x00007f61761fc000)<br>   libavfilter.so.5 =&gt; /usr/local/ffmpeg/lib/libavfilter.so.5 (0x00007f6175e70000)<br>   libavformat.so.56 =&gt; /usr/local/ffmpeg/lib/libavformat.so.56 (0x00007f6175a8c000)<br>   libavcodec.so.56 =&gt; /usr/local/ffmpeg/lib/libavcodec.so.56 (0x00007f61745eb000)<br>   libpostproc.so.53 =&gt; not found<br>   libswresample.so.1 =&gt; /usr/local/ffmpeg/lib/libswresample.so.1 (0x00007f61743cf000)<br>   libswscale.so.3 =&gt; /usr/local/ffmpeg/lib/libswscale.so.3 (0x00007f617413e000)<br>   libavutil.so.54 =&gt; /usr/local/ffmpeg/lib/libavutil.so.54 (0x00007f6173ed7000)<br>   libm.so.6 =&gt; /lib64/libm.so.6 (0x00007f6173c52000)<br>   libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00007f6173a35000)<br>   libc.so.6 =&gt; /lib64/libc.so.6 (0x00007f61736a1000)<br>   libpostproc.so.53 =&gt; not found<br>   libz.so.1 =&gt; /lib64/libz.so.1 (0x00007f617348a000)<br>   libx264.so.148 =&gt; not found<br>   librt.so.1 =&gt; /lib64/librt.so.1 (0x00007f6173281000)<br>   /lib64/ld-linux-x86-64.so.2 (0x00007f617641c000)<br>可以看到libpostproc.so.53和libx264.so.148 not found<br>出现这类错误表示，系统不知道xxx.so放在哪个目录下，这时候就要在/etc/ld.so.conf中加入xxx.so所在的目录。<br>一般而言，有很多的so会存放在/usr/local/lib这个目录底下，去这个目录底下找，果然发现自己所需要的.so文件。<br>所以，在/etc/ld.so.conf中加入/usr/local/lib这一行，保存之后，再运行：/sbin/ldconfig –v(或者直接运行ldconfig命令)更新一下配置即可。</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;下载源码&lt;br&gt;  libx264 &lt;a href=&quot;http://download.videolan.org/x264/snapshots/&quot;&gt;http://download.videolan.org/x264/snapshots/&lt;/a&gt; 最新版&lt;br&gt;  yasm &lt;a href=&quot;http://yasm.tortall.net/Download.html&quot;&gt;http://yasm.tortall.net/Download.html&lt;/a&gt; 版本为yasm-1.3.0(yasm是汇编编译器,因为ffmpeg中为了提高效率用到了汇编指令)&lt;br&gt;  ffmpeg &lt;a href=&quot;http://www.ffmpeg.org/download.html&quot;&gt;http://www.ffmpeg.org/download.html&lt;/a&gt; 版本为ffmpge-2.6.3&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yzy755.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="ffmpeg" scheme="http://yzy755.github.io/tags/ffmpeg/"/>
    
      <category term="编译" scheme="http://yzy755.github.io/tags/%E7%BC%96%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>hexo编辑博客整理</title>
    <link href="http://yzy755.github.io/2017/03/08/hexo%E7%BC%96%E8%BE%91%E5%8D%9A%E5%AE%A2%E6%95%B4%E7%90%86/"/>
    <id>http://yzy755.github.io/2017/03/08/hexo编辑博客整理/</id>
    <published>2017-03-08T07:56:48.000Z</published>
    <updated>2017-03-09T09:51:32.491Z</updated>
    
    <content type="html"><![CDATA[<p>一.Hexo搭建Github-Pages博客<br>   1.安装nodejs和git,方法略</p>
<p>   2.安装hexo<br>       $ npm install -g hexo<br><a id="more"></a><br>   3.部署hexo<br>       $ hexo init<br>       .<br>        ├── .deploy<br>        ├── public<br>        ├── scaffolds<br>        ├── scripts<br>        ├── source<br>        |   ├── _drafts<br>        |   └── _posts<br>        ├── themes<br>        ├── _config.yml<br>        └── package.json<br>        .deploy：执行hexo deploy命令部署到GitHub上的内容目录<br>        public：执行hexo generate命令，输出的静态网页内容目录<br>        scaffolds：layout模板文件目录，其中的md文件可以添加编辑<br>        scripts：扩展脚本目录，这里可以自定义一些javascript脚本<br>        source：文章源码目录，该目录下的markdown和html文件均会被hexo处理。该页面对应repo的根目录，404文件、favicon.ico文件，CNAME文件等都应该放这里，该目录下可新建页面目录。<br>        _drafts：草稿文章<br>        _posts：发布文章<br>        themes：主题文件目录<br>        _config.yml：全局配置文件，大多数的设置都在这里<br>        package.json：应用程序数据，指明hexo的版本等信息，类似于一般软件中的关于按钮</p>
<pre><code>4.Hexo命令
    Hexo下，通过 _config.yml 设置博客，可以想象成我们用的软件里的设置一样，只是它通过一个文件列出这些参数，然后让我们填写和修改。

    全局设置
    在你博客目录下有一个文件名_config.yml，打开可以配置信息。

    局部页面
    在你博客目录下 \themes\你使用的主题\_config.yml

    写博客相关命令
        Hexo常用命令：
        hexo new &quot;postName&quot;       #新建文章
        hexo new page &quot;pageName&quot;  #新建页面
        hexo generate             #生成静态页面至public目录
        hexo server               #开启预览访问端口（默认端口4000，&apos;ctrl + c&apos;关闭server）
        hexo deploy               #将.deploy目录部署到GitHub

    当然，如果每次输入那么长命令，那么一定想到用简写：
        hexo n == hexo new
        hexo g == hexo generate
        hexo s == hexo server
        hexo d == hexo deploy

    其它的，还可以复合命令：
        hexo deploy -g
        hexo server -g

    有时候生成的网页出错了，而生成的rss其实没有清除，那么用下面的命令，在重新生成吧
        $ hexo clean

    当本地调试出现诡异现象时候，请先使用 hexo clean 清理已经生成的静态文件后重试。

    注：Hexo原理就是hexo在执行hexo generate时会在本地先把博客生成的一套静态站点放到public文件夹中，在执行hexo deploy时将其复制到.deploy文件夹中。Github的版本库通常建议同时附上README.md说明文件，但是hexo默认情况下会把所有md文件解析成html文件，所以即使你在线生成了README.md，它也会在你下一次部署时被删去。怎么解决呢？
    在执行hexo deploy前把在本地写好的README.md文件复制到.deploy文件夹中，再去执行hexo deploy。

5.博客管理
    上面命令中，其实生成文章，可以直接把写好的文章插入到目录/_posts 下面，后缀为.MD就行，在文章头部固定格式：
        title: Mac提高使用效率的一些方法   #文章的标题，这个才是显示的文章标题，其实文件名不影响
        date: 2015-09-01 20:33:26      #用命令会自动生成，也可以自己写，所以文章时间可以改
        categories: technology         #文章的分类，这个可以自己定义
        tags: [Mac,效率,快捷方式]        #tag，为文章添加标签，方便搜索
        ---
    当然，里面有很多东西的，如果你专注于写作，那么可以不用太关心了，比如tags标签可以写成下面那样，因为hexo文章的头部文件是用AML来写的。
        tags:
        - tag1
        - tag2
    如果在博客文章列表中，不想全文显示，可以增加 &lt;!--more--&gt;, 后面的内容就不会显示在列表。
         &lt;!--more--&gt;

6.插件
    安装插件
      $ npm install &lt;plugin-name&gt; --save
    添加RSS

      npm install hexo-generator-feed
    然后，到博客目录 /public 下，如果没有发现atom.xml，说明命令没有生效！！！(楼主就是在这里被坑了次)
    解决方法：
      $ npm install hexo-generator-feed --save
    这个命令来自hexo-generator-feed

      Install
         $ npm install hexo-generator-feed --save
       Hexo 3: 1.x
       Hexo 2: 0.x
      Options
         You can configure this plugin in _config.yml.

        feed:
           type: atom
           path: atom.xml
           limit: 20

          type - Feed type. (atom/rss2)
          path - Feed path. (Default: atom.xml/rss2.xml)
          limit - Maximum number of posts in the feed (Use 0 or false to show all posts)
    其中可以选择：
    然后在 Hexo 根目录下的 _config.yml 里配置一下

        feed:
            type: atom
            path: atom.xml
            limit: 20
        #type 表示类型, 是 atom 还是 rss2.
        #path 表示 Feed 路径
        #limit 最多多少篇最近文章
    最后，在 hexo generate之后，会发现public文件夹下多了atom.xml！

    例如要订阅我的blog只要输入ihtc.cc/atom就可以搜寻到啦！

    添加Sitemap
    Sitemap 的提交主要的目的，是要避免搜索引擎的爬虫没有完整的收录整个网页的内容，所以提交 Sitemap 是能够补足搜索引擎的不足，进而加速网页的收录速度，达到搜寻引擎友好的目的。
        $ npm install hexo-generator-sitemap --save
    这个命令来自hexo-generator-sitemap

        Install
            $ npm install hexo-generator-sitemap --save

            Hexo 3: 1.x
            Hexo 2: 0.x
        Options
            You can configure this plugin in _config.yml.

            sitemap:
                path: sitemap.xml
            path - Sitemap path. (Default: sitemap.xml)
    同样可以选择：
    在 Hexo 根目录下的 _config.yml 里配置一下

            sitemap:
               path: sitemap.xml
               #path 表示 Sitemap 的路径. 默认为 sitemap.xml.
    对于国内用户还需要安装插件 hexo-generator-baidu-sitemap, 顾名思义是为百度量身打造的. 安装

            $ npm install hexo-generator-baidu-sitemap --save
    然后在 Hexo 根目录下的 _config.yml 里配置一下

           baidusitemap:
                path: baidusitemap.xml
    为了博客有更好的展示率, 最好的方式是通过搜索引擎, 提交 Sitemap文件是一个方式，具体可参考：

    Hexo 优化与定制(二) | Kang Lu&apos;s Blog
    ｜Hexo优化｜如何向google提交sitemap（详细） | Fiona&apos;s Blog

    其它插件
    Plugins · hexojs/hexo

7.评论设置
    在Hexo中，默认使用的评论是国外的Disqus,不过因为国内的”网络环境”问题，我们改为国内的多说评论系统。
    需要说明的是 short_name:字段，这个字段为你多说填写的站点名字，比如我的域名：ihtcboy.duoshuo.com，那么我的short_name:&quot;ihtcboy&quot;

8.404页面
    GitHub Pages 自定义404页面非常容易，直接在根目录下创建自己的404.html就可以。但是自定义404页面仅对绑定顶级域名的项目才起作用，GitHub默认分配的二级域名是不起作用的，使用hexo server在本机调试也是不起作用的。
    其实，404页面可以做更多有意义的事，来做个404公益项目吧。
    腾讯公益 404.html :

        &lt;html&gt;
        &lt;head&gt;   
            &lt;meta charset=&quot;UTF-8&quot;&gt;
            &lt;title&gt;404&lt;/title&gt;
        &lt;/head&gt;
        &lt;body&gt;
        &lt;br&gt;&lt;!--
        &lt;!DOCTYPE HTML&gt;
        &lt;html&gt;
        &lt;head&gt;
            &lt;meta charset=&quot;UTF-8&quot; /&gt;
            &lt;title&gt;公益404 | 不如&lt;/title&gt;
        &lt;/head&gt;
        &lt;body&gt;
        #404 Not found By Bruce
        &lt;h1&gt;404 Page Not Found&lt;/h1&gt;
        --&gt;&lt;br&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;http://www.qq.com/404/search_children.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;br&gt;&lt;!--
        公益404介接入地址
        益云公益404 http://yibo.iyiyun.com/Index/web404
        腾讯公益404 http://www.qq.com/404
        失蹤兒童少年資料管理中心404 http://404page.missingkids.org.tw
        --&gt;
        &lt;br&gt;
        &lt;/body&gt;
        &lt;/html&gt;
    复制上面代码，贴粘到目录下新建的404.html即可！

9.统计
    因Google Analytics偶尔被墙，故国内用百度统计
    最新的统计服务已经开放，两行代码轻松搞定，你可以直接使用：不蒜子
    本人墙裂推荐，只需要两行代码哦。各种用法实例和显示效果参考不蒜子文档中的实例链接。不蒜子，极客的算子，极简的算子，任你发挥的算子。

10.更新
    更新hexo：
      npm update -g hexo
    更新主题：
      cd themes/你的主题
      git pull
    更新插件：
      npm update

11.在 hexo 中无痛使用本地图片
    首先确认 _config.yml 中有 post_asset_folder:true

    在 hexo 目录，执行

    npm install https://github.com/CodeFalling/hexo-asset-image --save

    假设在
    MacGesture2-Publish
    ├── apppicker.jpg
    ├── logo.jpg
    └── rules.jpg
    MacGesture2-Publish.md
    这样的目录结构（目录名和文章名一致），只要使用 ![logo](MacGesture2-Publish/logo.jpg) 就可以插入图片。

    生成的结构为
    public/2015/10/18/MacGesture2-Publish
    ├── apppicker.jpg
    ├── index.html
    ├── logo.jpg
    └── rules.jpg
    同时，生成的 html 是
    &lt;img src=&quot;/2015/10/18/MacGesture2-Publish/logo.jpg&quot; alt=&quot;logo&quot;&gt;

12.添加评论插件
    在_config.yml中添加多说的配置：
      duoshuo_shortname: 你站点的short_name ps:只需要写你注册填写的内容

      修改themes\landscape\layout\_partial\article.ejs模板
    把

      &lt;% if (!index &amp;&amp; post.comments &amp;&amp; config.disqus_shortname){ %&gt;
      &lt;section id=&quot;comments&quot;&gt;
        &lt;div id=&quot;disqus_thread&quot;&gt;
          &lt;noscript&gt;Please enable JavaScript to view the &lt;a href=&quot;//disqus.com/?ref_noscript&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;
        &lt;/div&gt;
      &lt;/section&gt;
      &lt;% } %&gt;
    改为

      &lt;% if (!index &amp;&amp; post.comments &amp;&amp; config.duoshuo_shortname){ %&gt;
      &lt;section id=&quot;comments&quot;&gt;
        &lt;!-- 多说评论框 start --&gt;
        &lt;div class=&quot;ds-thread&quot; data-thread-key=&quot;&lt;%= post.layout %&gt;-&lt;%= post.slug %&gt;&quot; data-title=&quot;&lt;%= post.title %&gt;&quot; data-url=&quot;&lt;%= page.permalink %&gt;&quot;&gt;&lt;/div&gt;
        &lt;!-- 多说评论框 end --&gt;
        &lt;!-- 多说公共JS代码 start (一个网页只需插入一次) --&gt;
        &lt;script type=&quot;text/javascript&quot;&gt;
        var duoshuoQuery = {short_name:&apos;&lt;%= config.duoshuo_shortname %&gt;&apos;};
          (function() {
            var ds = document.createElement(&apos;script&apos;);
            ds.type = &apos;text/javascript&apos;;ds.async = true;
            ds.src = (document.location.protocol == &apos;https:&apos; ? &apos;https:&apos; : &apos;http:&apos;) + &apos;//static.duoshuo.com/embed.js&apos;;
            ds.charset = &apos;UTF-8&apos;;
            (document.getElementsByTagName(&apos;head&apos;)[0] 
             || document.getElementsByTagName(&apos;body&apos;)[0]).appendChild(ds);
          })();
          &lt;/script&gt;
        &lt;!-- 多说公共JS代码 end --&gt;
      &lt;/section&gt;
      &lt;% } %&gt;
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一.Hexo搭建Github-Pages博客&lt;br&gt;   1.安装nodejs和git,方法略&lt;/p&gt;
&lt;p&gt;   2.安装hexo&lt;br&gt;       $ npm install -g hexo&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yzy755.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="hexo" scheme="http://yzy755.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>zookeeper浅读</title>
    <link href="http://yzy755.github.io/2017/03/08/zookeeper%E6%B5%85%E8%AF%BB/"/>
    <id>http://yzy755.github.io/2017/03/08/zookeeper浅读/</id>
    <published>2017-03-08T07:44:27.000Z</published>
    <updated>2017-03-09T09:12:33.717Z</updated>
    
    <content type="html"><![CDATA[<p>下面我们来看下开源dubbo推荐的业界成熟的zookeeper做为注册中心， zookeeper是hadoop的一个子项目是分布式系统的可靠协调者，他提供了配置维护，名字服务，分布式同步等服务。对于zookeeper的原理本文档不分析，后面有时间在做专题。<br>zookeeper注册中心<br>Zookeeper对数据存储类似linux的目录结构，下面给出官方文档对dubbo注册数据的存储示例<br><a id="more"></a><br><img src="/2017/03/08/zookeeper浅读/1.png" alt="logo"></p>
<p>假设读者对zookeeper有所了解，能够搭建zookeeper服务，其实不了解也没关系，谷歌百度下分分钟搞起。<br>作为测试调试dubbo，我是在本地起的zookeeper<br><img src="/2017/03/08/zookeeper浅读/2.png" alt="logo"></p>
<p>指定zookeeper配置文件地址<br><img src="/2017/03/08/zookeeper浅读/3.png" alt="logo"></p>
<p>配置文件中两个关键参数：<br>dataDir zookeeper存储文件的地址<br>clientPort 客户端链接的端口号<br>Dubbo服务提供者配置</p>
<p><dubbo:registry protocol="”" zookeeper="" ”="" address="127.0.0. 1: 2181 "></dubbo:registry></p>
<p><beanid="demoservice" class="com.alibaba.dubbo.demo.provi der.DemoServiceImpl"></beanid="demoservice"></p>
<p><dubbo:serviceinterface="com.alibaba.dubbo.demo.demoservi ce"="" ref="demoService"><br>除了配置注册中心的，其他都一样<br>Dubbo服务消费者配置</dubbo:serviceinterface="com.alibaba.dubbo.demo.demoservi></p>
<p><dubbo:registry protocol="”" zookeeper="" ”="" address="127.0.0. 1: 2181 "></dubbo:registry></p>
<p><dubbo:reference id="demoService" interface="com.alibaba.dubbo.demo.DemoService"><br>除了配置注册中心的，其他都一样<br>客户端获取注册器<br>服务的提供者和消费者在RegistryProtocol利用注册中心暴露(export)和引用(refer)服务的时候会根据配置利用Dubbo的SPI机制获取具体注册中心注册器<br>Registry registry = registryFactory .getRegistry(url);<br>这里的RegistryFactory是ZookeeperRegistryFactory看如下工厂代码<br>public class ZookeeperRegistryFactory extends AbstractRegistryFactory {<br>    public Registry createRegistry(URL url) {<br>        return new ZookeeperRegistry(url, zookeeperTransporter );<br>    }<br>}<br>这里创建zookeepr注册器ZookeeperRegistry<br>ZookeeperTransporter是操作zookeepr的客户端的工厂类，用来创建zookeeper客户端，这里客户端并不是zookeeper源代码的自带的，而是采用第三方工具包，主要来简化对zookeeper的操作，例如用zookeeper做注册中心需要对zookeeper节点添加watcher做反向推送，但是每次回调后节点的watcher都会被删除，这些客户会自动维护了这些watcher，在自动添加到节点上去。<br>接口定义：<br>@SPI ( “zkclient” )<br>public interface ZookeeperTransporter {<br>    @Adaptive ({Constants. CLIENT_KEY , Constants. TRANSPORTER_KEY })<br>    ZookeeperClient connect(URL url);<br>}<br>默认采用zkClient， dubbo源码集成两种zookeeper客户端，除了zkClient还有一个是curator<br><img src="/2017/03/08/zookeeper浅读/4.png" alt="logo"></dubbo:reference></p>
<p>ZookeeperRegistry注册器的实现<br>1.构造器利用客户端创建了对zookeeper的连接，并且添加了自动回复连接的监听器。<br>zkClient = zookeeperTransporter.connect(url);<br>zkClient .addStateListener( new StateListener() {<br>    public void stateChanged( int state) {<br>    if (state == RECONNECTED )<br>        recover();<br>    }<br>});</p>
<p>2.注册url就是利用客户端在服务器端创建url的节点，默认为临时节点，客户端与服务端断开，几点自动删除<br>zkClient .create(toUrlPath(url),url.getParameter(Constants. DYNAMIC_KEY , true));      </p>
<p>3.取消注册的url，就是利用zookeeper客户端删除url节点<br>zkClient .delete(toUrlPath(url));</p>
<ol>
<li><p>订阅url， 功能是服务消费端订阅服务提供方在zookeeper上注册地址，这个功能流程跟DubboRegister不一样， DubboRegister是通过Dubbo注册中心实现SimpleResgiter在注册中心端，对url变换、过滤筛选然后将获取的provierUrl(提供者ulr)利用服务消费者暴露的服务回调在refer。<br>由于这里注册中心采用的是zookeeper，zookeeper不可能具有dubbo的业务逻辑，这里对订阅的逻辑处理都在消费服务端订阅的时候处理。<br> 1) 对传入url的serviceInterface是*代表订阅url目录下所有节点即所有服务，这个注册中心需要订阅所有<br> 2) 如果指定了订阅接口通过toCategoriesPath(url)转换需要订阅的url<br>  如传入url consumer://10.33.37.8/com.alibaba.dubbo.demo.DemoService?application=demo-consumer&amp;category=providers,configurators,routers&amp;dubbo=2.5.4-SNAPSHOT&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4088&amp;side=consumer&amp;timestamp=1417405597808<br>  转换成urls<br>  /dubbo/com.alibaba.dubbo.demo.DemoService/providers,/dubbo/com.alibaba.dubbo.demo.DemoService/configurators, /dubbo/com.alibaba.dubbo.demo.DemoService/routers<br> 3) 设配传入的回调接口NotifyListener,转换成dubbo对zookeeper操作的ChildListener<br> 4) 以/dubbo/com.alibaba.dubbo.demo.DemoService/providers为例创建节点zkClient.create(path, false);<br>  但是一般情况下如果服务提供者已经提供服务，那么这个目录节点应该已经存在，Dubbo在Client层屏蔽掉了创建异常。<br> 5) 以/dubbo/com.alibaba.dubbo.demo.DemoService/providers为例给节点添加监听器，返回所有子目录<br>  List<string> children = zkClient .addChildListener(path, zkListener);<br>  if (children != null ) {urls.addAll(toUrlsWithEmpty(url, path,hildren));}<br>  toUrlsWtihEmpty用来配置是不是需要订阅的url，是加入集合<br> 6） 主动根据得到服务提供者urls回调NotifyListener,引用服务提供者生成invoker可执行对象</string></p>
</li>
<li><p>取消订阅url， 只是去掉url上的注册的监听器</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;下面我们来看下开源dubbo推荐的业界成熟的zookeeper做为注册中心， zookeeper是hadoop的一个子项目是分布式系统的可靠协调者，他提供了配置维护，名字服务，分布式同步等服务。对于zookeeper的原理本文档不分析，后面有时间在做专题。&lt;br&gt;zookeeper注册中心&lt;br&gt;Zookeeper对数据存储类似linux的目录结构，下面给出官方文档对dubbo注册数据的存储示例&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="原理研究" scheme="http://yzy755.github.io/categories/%E5%8E%9F%E7%90%86%E7%A0%94%E7%A9%B6/"/>
    
    
      <category term="zookeeper" scheme="http://yzy755.github.io/tags/zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>TCP的几个状态码</title>
    <link href="http://yzy755.github.io/2017/03/08/TCP%E7%9A%84%E5%87%A0%E4%B8%AA%E7%8A%B6%E6%80%81%E7%A0%81/"/>
    <id>http://yzy755.github.io/2017/03/08/TCP的几个状态码/</id>
    <published>2017-03-08T03:48:05.000Z</published>
    <updated>2017-03-09T09:55:25.371Z</updated>
    
    <content type="html"><![CDATA[<p>在TCP层，有个FLAGS字段，这个字段有以下几个标识：SYN, FIN, ACK, PSH, RST, URG.</p>
<p>其中，对于我们日常的分析有用的就是前面的五个字段。</p>
<p>它们的含义是：</p>
<p>SYN表示建立连接，</p>
<p>FIN表示关闭连接，</p>
<p>ACK表示响应，</p>
<p>PSH表示有 DATA数据传输，</p>
<p>RST表示连接重置。<br><a id="more"></a></p>
<p>其中，ACK是可能与SYN，FIN等同时使用的，比如SYN和ACK可能同时为1，它表示的就是建立连接之后的响应，</p>
<p>如果只是单个的一个SYN，它表示的只是建立连接。</p>
<p>TCP的几次握手就是通过这样的ACK表现出来的。</p>
<p>但SYN与FIN是不会同时为1的，因为前者表示的是建立连接，而后者表示的是断开连接。</p>
<p>RST一般是在FIN之后才会出现为1的情况，表示的是连接重置。</p>
<p>一般地，当出现FIN包或RST包时，我们便认为客户端与服务器端断开了连接；而当出现SYN和SYN＋ACK包时，我们认为客户端与服务器建立了一个连接。</p>
<p>PSH为1的情况，一般只出现在 DATA内容不为0的包中，也就是说PSH为1表示的是有真正的TCP数据包内容被传递。</p>
<p>TCP的连接建立和连接关闭，都是通过请求－响应的模式完成的。</p>
<p>概念补充-TCP三次握手：<br>TCP(Transmission Control Protocol)传输控制协议<br>TCP是主机对主机层的传输控制协议，提供可靠的连接服务，采用三次握手确认建立一个连接：</p>
<p>位码即tcp标志位，有6种标示：SYN(synchronous建立联机) ACK(acknowledgement 确认) PSH(push传送) FIN(finish结束) RST(reset重置) URG(urgent紧急)Sequence number(顺序号码) Acknowledge number(确认号码)<br>第一次握手：主机A发送位码为syn＝1，随机产生seq number=1234567的数据包到服务器，主机B由SYN=1知道，A要求建立联机；<br>第二次握手：主机B收到请求后要确认联机信息，向A发送ack number=(主机A的seq+1)，syn=1，ack=1，随机产生seq=7654321的包；<br>第三次握手：主机A收到后检查ack number是否正确，即第一次发送的seq number+1，以及位码ack是否为1，若正确，主机A会再发送ack number=(主机B的seq+1)，ack=1，主机B收到后确认seq值与ack=1则连接建立成功。<br>完成三次握手，主机A与主机B开始传送数据。<br>TCP采用三次握手建立连接如下图所示。<br><img src="/2017/03/08/TCP的几个状态码/connect.png" alt="logo"><br>用大白话就是这样：<br><img src="/2017/03/08/TCP的几个状态码/connect2.png" alt="logo"></p>
<p>在TCP/IP协议中，TCP协议提供可靠的连接服务，采用三次握手建立一个连接。<br>第一次握手：建立连接时，客户端发送syn包(syn=j)到服务器，并进入SYN_SEND状态，等待服务器确认；<br>第二次握手：服务器收到syn包，必须确认客户的SYN（ack=j+1），同时自己也发送一个SYN包（syn=k），即SYN+ACK包，此时服务器进入SYN_RECV状态；<br>第三次握手：客户端收到服务器的SYN＋ACK包，向服务器发送确认包ACK(ack=k+1)，此包发送完毕，客户端和服务器进入ESTABLISHED状态，完成三次握手。完成三次握手，客户端与服务器开始传送数据.</p>
<p>关闭连接：<br>由于TCP连接是全双工的，因此每个方向都必须单独进行关闭。这个原则是当一方完成它的数据发送任务后就能发送一个FIN来终止这个方向的连接。收到一个 FIN只意味着这一方向上没有数据流动，一个TCP连接在收到一个FIN后仍能发送数据。首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。</p>
<p> CP的连接的拆除需要发送四个包，因此称为四次挥手(four-way handshake)。客户端或服务器均可主动发起挥手动作，在socket编程中，任何一方执行close()操作即可产生挥手操作。</p>
<p>（1）客户端A发送一个FIN，用来关闭客户A到服务器B的数据传送。 </p>
<p>（2）服务器B收到这个FIN，它发回一个ACK，确认序号为收到的序号加1。和SYN一样，一个FIN将占用一个序号。 </p>
<p>（3）服务器B关闭与客户端A的连接，发送一个FIN给客户端A。 </p>
<p>（4）客户端A发回ACK报文确认，并将确认序号设置为收到序号加1。 </p>
<p>TCP采用四次挥手关闭连接如下图所示。<br><img src="/2017/03/08/TCP的几个状态码/close.png" alt="logo"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在TCP层，有个FLAGS字段，这个字段有以下几个标识：SYN, FIN, ACK, PSH, RST, URG.&lt;/p&gt;
&lt;p&gt;其中，对于我们日常的分析有用的就是前面的五个字段。&lt;/p&gt;
&lt;p&gt;它们的含义是：&lt;/p&gt;
&lt;p&gt;SYN表示建立连接，&lt;/p&gt;
&lt;p&gt;FIN表示关闭连接，&lt;/p&gt;
&lt;p&gt;ACK表示响应，&lt;/p&gt;
&lt;p&gt;PSH表示有 DATA数据传输，&lt;/p&gt;
&lt;p&gt;RST表示连接重置。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yzy755.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="tcp" scheme="http://yzy755.github.io/tags/tcp/"/>
    
      <category term="三次握手" scheme="http://yzy755.github.io/tags/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B/"/>
    
      <category term="四次挥手" scheme="http://yzy755.github.io/tags/%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/"/>
    
  </entry>
  
  <entry>
    <title>微信小程序对接支付</title>
    <link href="http://yzy755.github.io/2017/03/02/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%AF%B9%E6%8E%A5%E6%94%AF%E4%BB%98/"/>
    <id>http://yzy755.github.io/2017/03/02/微信小程序对接支付/</id>
    <published>2017-03-02T09:17:53.000Z</published>
    <updated>2017-04-11T09:28:05.484Z</updated>
    
    <content type="html"><![CDATA[<p>所有支付方式都需要通过 「统一下单」的 API 来获取一个支付凭证。<br>但在小程序内测期间，还没有「统一下单」的概念。HTML 5 应用发起支付需要直接通过前端构造参数来发起（不经过后端验证），很容易造成支付凭证泄露等安全问题。<br>为此，微信支付将其流程进行了优化：在所有支付场景中插入「统一下单」的特性。推荐开发者在后端完成支付参数的构建等行为。<br>该优化带来以下好处:</p>
<pre><code>* 尽可能让开发者不犯低级错误，造成财务损失。
* 简化构造支付参数的复杂度，所有支付方式可共享一个支付后端接口。
</code></pre><a id="more"></a>
<p>通过「统一下单」获取到相对应 prepay_id 或者 code_url 等参数，即可通过各种支付模式的 SDK 来进行微信支付的发起。<br>需要注意的是，必须对通知参数进行签名验证，以确保安全。<br>进行签名验证时，除去签名字段（一般参数名为: sign）不需要参与签名外，其余所有接收到的参数均需要参与签名。<br>比如拼接的请求支付宝的参数为total_fee,body,appid，那么签名就必须用这三个参数<br>得到prepay_id后还需在发起wx.requestPayment前对 wx.requestPayment请求所需的参数进行签名<br>所需的参数为’timeStamp’: res.data.timeStamp,<br>            ‘nonceStr’: nonce_str,<br>            ‘package’: ‘prepay_id=’+prepay_id,<br>            ‘signType’: ‘MD5’,<br>            ‘paySign’: res.data.sign,<br>但是后台生成该签名还需appId（I大写！！）随后用返回的参数和生成的timeStamp进行请求（ appId、timeStamp、nonceStr、package、signType）。</p>
<p>之后又将微信支付集成到了web端，官方对于网页版的微信支付有两种模式，我使用的是更为简单的模式二，参考<a href="https://pay.weixin.qq.com/wiki/doc/api/native.php?chapter=6_5" target="_blank" rel="external">https://pay.weixin.qq.com/wiki/doc/api/native.php?chapter=6_5</a><br>时序图如下：<br><img src="/2017/03/02/微信小程序对接支付/timing.png" alt="logo"><br>与小程序的统一下单api所需的参数不同的是：小程序的trade_type一定是”JSAPI”,相应的一定要传当前使用小程序的用户openid；扫码支付模式二的trade_type是”NATIVE”，不需要用户的openid,其他参数相同，为：<br>        wxRequestParam.setAppid(ConstantUtil.APP_ID);<br>        wxRequestParam.setMch_id(ConstantUtil.MCH_ID);<br>        wxRequestParam.setNonce_str(WXUtil.getNonceStr());<br>        wxRequestParam.setBody(commonsOrder.getTitle());<br>        wxRequestParam.setSpbill_create_ip(request.getRemoteAddr().startsWith(“0”)?”127.0.0.1”:request.getRemoteAddr());<br>        wxRequestParam.setOut_trade_no(commonsOrder.getOrderCode());<br>        wxRequestParam.setTotal_fee(StringUtils.substringBefore(String.valueOf(commonsOrder.getOrderFee().setScale(2, BigDecimal.ROUND_HALF_UP).doubleValue() * 100), “.”));<br>        wxRequestParam.setNotify_url(ConstantUtil.NOTIFY_URL);<br>        wxRequestParam.setSign(sign);<br>        wxRequestParam.setTrade_type(“NATIVE”);<br>        // 下面是小程序不同的参数<br>        wxRequestParam.setOpenid(openid);<br>        wxRequestParam.setTrade_type(“JSAPI”);</p>
<p>统一下单后返回参数中有一个code_url，使用 <a href="http://qr.liantu.com/api.php?text=code_url" target="_blank" rel="external">http://qr.liantu.com/api.php?text=code_url</a> 即可看到一张二维码，扫码就支付啦，是不是比较简单- -</p>
<p>——————————————–2017.4.6更新——————————————————<br>老大说使用其他网站平台生成二维码感觉不靠谱，所以去网上看了下有没有自己“画”二维码的库类，当然是有的，google爸爸就有一套基于nio的图片制作工具：<br>添加依赖：</p>
<pre><code>&lt;dependency&gt;  
    &lt;groupId&gt;com.google.zxing&lt;/groupId&gt;  
    &lt;artifactId&gt;core&lt;/artifactId&gt;  
    &lt;version&gt;3.0.0&lt;/version&gt;  
&lt;/dependency&gt;  
&lt;dependency&gt;  
    &lt;groupId&gt;com.google.zxing&lt;/groupId&gt;  
    &lt;artifactId&gt;javase&lt;/artifactId&gt;  
    &lt;version&gt;3.0.0&lt;/version&gt;  
&lt;/dependency&gt; 
</code></pre><p>编写工具类，代码如下：<br>    /**</p>
<pre><code> * @param qrImagePath 生成的图片绝对路径
 * @param qrImageName 生成的图片名称
 * @param content 待扫描的内容
 * @throws WriterException
 * @throws IOException
 */
public static String createQRcode(String qrImagePath, String qrImageName,
        String content) throws WriterException, IOException {
    int width = 300; // 图像宽度
    int height = 300; // 图像高度
    String format = &quot;png&quot;;// 图像类型
    Map&lt;EncodeHintType, Object&gt; hints = new HashMap&lt;EncodeHintType, Object&gt;();
    hints.put(EncodeHintType.CHARACTER_SET, &quot;UTF-8&quot;);
    BitMatrix bitMatrix = new MultiFormatWriter().encode(content,
            BarcodeFormat.QR_CODE, width, height, hints);// 生成矩阵
    Path path = FileSystems.getDefault().getPath(qrImagePath, qrImageName);
    MatrixToImageWriter.writeToPath(bitMatrix, format, path);// 输出图像
    return path.toString();
}

// 原来的生成方法
public static String QRfromLiantu(String chl) throws Exception {
    chl = UrlEncode(chl);
    String QRfromLiantu = &quot;http://qr.liantu.com/api.php?text=&quot; + chl;
    return QRfromLiantu;
}

// 特殊字符处理
public static String UrlEncode(String src)
        throws UnsupportedEncodingException {
    return URLEncoder.encode(src, &quot;UTF-8&quot;).replace(&quot;+&quot;, &quot;%20&quot;);
}
</code></pre><p>然后调用就简单啦，消除领导的顾虑<br>看了下京东的微信二维码支付，一分钟刷新一次，有自己独立的二维码服务，后期抽空研究下。。。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;所有支付方式都需要通过 「统一下单」的 API 来获取一个支付凭证。&lt;br&gt;但在小程序内测期间，还没有「统一下单」的概念。HTML 5 应用发起支付需要直接通过前端构造参数来发起（不经过后端验证），很容易造成支付凭证泄露等安全问题。&lt;br&gt;为此，微信支付将其流程进行了优化：在所有支付场景中插入「统一下单」的特性。推荐开发者在后端完成支付参数的构建等行为。&lt;br&gt;该优化带来以下好处:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;* 尽可能让开发者不犯低级错误，造成财务损失。
* 简化构造支付参数的复杂度，所有支付方式可共享一个支付后端接口。
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yzy755.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="微信小程序" scheme="http://yzy755.github.io/tags/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"/>
    
      <category term="微信支付" scheme="http://yzy755.github.io/tags/%E5%BE%AE%E4%BF%A1%E6%94%AF%E4%BB%98/"/>
    
  </entry>
  
  <entry>
    <title>solr部署</title>
    <link href="http://yzy755.github.io/2017/03/02/solr%E9%83%A8%E7%BD%B2/"/>
    <id>http://yzy755.github.io/2017/03/02/solr部署/</id>
    <published>2017-03-02T09:17:53.000Z</published>
    <updated>2017-03-09T09:12:45.411Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://www.tuicool.com/articles/mueARf" target="_blank" rel="external">http://www.tuicool.com/articles/mueARf</a> 参考</p>
<p>请下载tgz包而不是zip包，特此提醒！ 解压solr-5.1.0.zip到任意盘符，如图：</p>
<p>复制solr.war到tomcat的webapps目录下，如图操作：</p>
<p>启动tomcat,如图：<br><a id="more"></a><br>然后修改webapps\solr\WEB-INF下的web.xml配置文件，如图：</p>
<p>配置solr_home目录路径，如图：</p>
<p>然后我们需要去C盘创建一个solr_home文件夹，这就是我们SOLR_HOME根目录啦，如图：</p>
<p>然后我们需要把E:\solr-5.1.0\server\solr目录下的所有文件及文件夹全部复制到我们刚刚创建的solr_home目录下，如图：</p>
<p>然后我们需要把E:\solr-5.1.0\server\lib\ext目录下的所有jar包 copy到E:\apache-tomcat-7.0.55\webapps\solr\WEB-INF\lib目录下(<br>E:\apache-tomcat-7.0.55是我的tomcat安装根目录，请对照你们自己的tomcat安装目录进行调整，你懂的<br>)，如图：</p>
<p>然后复制 E:\solr-5.1.0\server\resources目录下的log4j.properties配置文件到E:\apache-tomcat-7.0.55\webapps\solr\WEB-INF目录下，如图操作：</p>
<p>然后重启我们的tomcat,如图：</p>
<p>打开你的浏览器，地址栏输入 <a href="http://localhost:8080/solr" target="_blank" rel="external">http://localhost:8080/solr</a> ， 访问我们的Solr Web后台。如果你能看到这个界面，即表明Solr5部署成功了，如图：</p>
<p>然后你就可以通过Solr Web UI添加Core啦，不过添加Core之前，你需要在solr_home目录下创建core文件夹，如图：<br>core目录下需要创建conf和data文件夹，你懂的， 《跟益达学Solr5之使用Jetty部署Solr》<br>这篇博客也提到过，如图：</p>
<p>剩下的一些配置copy具体你们就参照那篇去操作把，这里就不重复说明了。<br>到此，Solr5如何部署到Tomcat就讲解完毕了！</p>
<p>IK 分词器安装<br>1.将之前解压的solr-4.3.1 下的contrib和dist 文件夹复制到F:\winsolr\solr_home\solr\collection1下<br>2.将下载的IKAnalyzer的发行包解压，解压后将IKAnalyzer2012FF_u1.jar（分词器jar包）复制到F:\winsolr\solr_home\solr\collection1\contrib\analysis-extras\lib下<br>3.在F:\winsolr\apache-tomcat-7.0.37\webapps\solr\WEB-INF下新建classes文件夹<br>4.将IKAnalyzer解压出来的IKAnalyzer.cfg.xml（分词器配置文件）和 Stopword.dic（分词器停词字典,可自定义添加内容）复制到<br>  F:\winsolr\apache-tomcat-7.0.37\webapps\solr\WEB-INF\classes中<br>5.在F:\winsolr\solr_home\solr\collection1\conf下的schema.xml文件中fieldType name=”text_general”这个地方的上方添加以下内容 </p>
<p>将pinyinfj.jar、lucene-analyzers-smartcn-4.7.0.jar、IKAnalyzer2012FF_u1.jar、pinyinAnalyzer.jar、shentong_tsearch_lib.jar放到solr部署目录的web-inf的lib目录下</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://www.tuicool.com/articles/mueARf&quot;&gt;http://www.tuicool.com/articles/mueARf&lt;/a&gt; 参考&lt;/p&gt;
&lt;p&gt;请下载tgz包而不是zip包，特此提醒！ 解压solr-5.1.0.zip到任意盘符，如图：&lt;/p&gt;
&lt;p&gt;复制solr.war到tomcat的webapps目录下，如图操作：&lt;/p&gt;
&lt;p&gt;启动tomcat,如图：&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yzy755.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="solr" scheme="http://yzy755.github.io/tags/solr/"/>
    
  </entry>
  
  <entry>
    <title>jar包在windows下的运行命令</title>
    <link href="http://yzy755.github.io/2017/03/02/jar%E5%8C%85%E5%9C%A8windows%E4%B8%8B%E8%BF%90%E8%A1%8C%E5%91%BD%E4%BB%A4/"/>
    <id>http://yzy755.github.io/2017/03/02/jar包在windows下运行命令/</id>
    <published>2017-03-02T09:17:53.000Z</published>
    <updated>2017-03-09T09:13:47.771Z</updated>
    
    <content type="html"><![CDATA[<p>1、打jar包 jar -cvf xx.jar 文件夹名称</p>
<p>说明一下：<em>.</em>表示把当前目录下面以及子目录的所有class都打到这个xx.jar里。</p>
<p>-cvf的含义，可以自己去用jar命令去查看</p>
<p>如果要单独对某个或某些class文件进行打包，可以这样：</p>
<p>jar -cvf xx.jar Foo.class Bar.class </p>
<a id="more"></a>
<p>2.运行jar</p>
<p>java -jar xx.jar</p>
<p>要运行一个jar，则此jar内部的META-INF\MANIFEST.MF文件里必须指明要执行的main方法类</p>
<p>具体格式如：</p>
<p>Manifest-Version: 1.0<br>Created-By: 1.6.0_03 (Sun Microsystems Inc.)<br>Main-class: Test<br>如果此处的Test.class在com.xx包下面，则需要指明。</p>
<p>如果在运行时报了invalid or corrupt jarfile错误，则需要检查Main-class: Test 之间是不是缺少了空格。</p>
<p>3.指定运行jar里面的class</p>
<p>java -cp xx.jar com.xx.Test</p>
<p>4.编译某个java文件，但是依赖某个jar</p>
<p>javac -cp xx.jar Test.java</p>
<p> (Test.java里面import了xx.jar里面的某个class)</p>
<p>5.运行某个java文件，但是依赖某个jar</p>
<p>java -cp .;xx.jar Test</p>
<p>注意：引用xx.jar的时候，不要漏掉.;（这个表示当前目录）</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1、打jar包 jar -cvf xx.jar 文件夹名称&lt;/p&gt;
&lt;p&gt;说明一下：&lt;em&gt;.&lt;/em&gt;表示把当前目录下面以及子目录的所有class都打到这个xx.jar里。&lt;/p&gt;
&lt;p&gt;-cvf的含义，可以自己去用jar命令去查看&lt;/p&gt;
&lt;p&gt;如果要单独对某个或某些class文件进行打包，可以这样：&lt;/p&gt;
&lt;p&gt;jar -cvf xx.jar Foo.class Bar.class &lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yzy755.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="java" scheme="http://yzy755.github.io/tags/java/"/>
    
      <category term="jar包" scheme="http://yzy755.github.io/tags/jar%E5%8C%85/"/>
    
  </entry>
  
</feed>
